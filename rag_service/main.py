import os
import json
from datetime import datetime
from typing import List, Optional, Dict, Any
from pathlib import Path

from fastapi import FastAPI, HTTPException, UploadFile, File, Depends, Header
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ==================== LangChain æ ¸å¿ƒ ====================
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    PyPDFLoader, Docx2txtLoader, TextLoader
)
from langchain_community.vectorstores import Qdrant
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams

# ==================== LangChain Agents ====================
from langchain.agents import AgentExecutor, create_react_agent, Tool
from langchain.prompts import PromptTemplate

# ==================== LangChain Memory ====================
from langchain.memory import ConversationBufferMemory
from langchain_community.chat_message_histories import FileChatMessageHistory

# ==================== LangChain Chains ====================
from langchain.chains import (
    ConversationalRetrievalChain,
    LLMChain
)

from dotenv import load_dotenv
from loguru import logger

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# ==================== é…ç½® ====================
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", 6333))
ADMIN_PASSWORD = os.getenv("ADMIN_PASSWORD", "admin123456")
COLLECTION_NAME = "pais_knowledge_base"

# è¨­å®šæ—¥èªŒ
logger.add("logs/pais_{time}.log", rotation="1 day", retention="30 days")

# ==================== FastAPI æ‡‰ç”¨ ====================
app = FastAPI(
    title="PAIS - æ”¿å‹™åˆ†èº«æ™ºèƒ½ç³»çµ± (LangChain Powered)",
    description="åŸºæ–¼ LangChain çš„å¸‚é•·èŠå¤©æ©Ÿå™¨äºº (Agents + Memory + RAG + Tools)",
    version="2.0.2" # ç‰ˆæœ¬æ›´æ–°
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== LangChain åˆå§‹åŒ– ====================

# Gemini LLM
llm = GoogleGenerativeAI(
    model="gemini-2.0-flash",
    google_api_key=GEMINI_API_KEY,
    temperature=0.7,
    max_output_tokens=2048
)

# Embeddings (ä½¿ç”¨ moka-ai/m3e-base)
embeddings = HuggingFaceEmbeddings(
    model_name="moka-ai/m3e-base",
    model_kwargs={'device': 'cpu'}
)

# Qdrant å‘é‡è³‡æ–™åº«
qdrant_client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)

try:
    qdrant_client.get_collection(COLLECTION_NAME)
    logger.info(f"âœ… é›†åˆ '{COLLECTION_NAME}' å·²å­˜åœ¨")
except Exception:
    logger.warning(f"âš ï¸ é›†åˆ '{COLLECTION_NAME}' ä¸å­˜åœ¨ï¼Œå˜—è©¦å»ºç«‹...")
    try:
        qdrant_client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=768, distance=Distance.COSINE) # ç¶­åº¦ç‚º 768
        )
        logger.info(f"âœ… å·²å»ºç«‹æ–°é›†åˆ '{COLLECTION_NAME}'")
    except Exception as create_err:
        logger.error(f"âŒ å»ºç«‹é›†åˆ '{COLLECTION_NAME}' å¤±æ•—: {create_err}")
        # å¦‚æœç„¡æ³•å»ºç«‹é›†åˆï¼Œå¾ŒçºŒçš„ vectorstore åˆå§‹åŒ–å¯èƒ½æœƒå¤±æ•—

vectorstore = Qdrant(
    client=qdrant_client,
    collection_name=COLLECTION_NAME,
    embeddings=embeddings
)

# ==================== LangChain Memory ç®¡ç† ====================
memory_store: Dict[str, ConversationBufferMemory] = {}

def get_memory(session_id: str) -> ConversationBufferMemory:
    """å–å¾—æˆ–å»ºç«‹å°è©±è¨˜æ†¶"""
    if session_id not in memory_store:
        try:
            # ç¢ºä¿ chat_history ç›®éŒ„å­˜åœ¨
            Path("chat_history").mkdir(exist_ok=True)
            history_file = f"chat_history/{session_id}.json"
            message_history = FileChatMessageHistory(history_file)

            memory_store[session_id] = ConversationBufferMemory(
                chat_memory=message_history,
                memory_key="chat_history",
                return_messages=True
                # output_key æœƒåœ¨ chat å‡½æ•¸ä¸­å‹•æ…‹è¨­å®š
            )
            logger.info(f"ğŸ§  å»ºç«‹æ–°è¨˜æ†¶: {session_id} (æª”æ¡ˆ: {history_file})")
        except Exception as mem_err:
            logger.error(f"âŒ å»ºç«‹è¨˜æ†¶é«”å¤±æ•— ({session_id}): {mem_err}")
            # è¿”å›ä¸€å€‹è‡¨æ™‚çš„è¨˜æ†¶é«”ä»¥é¿å…å´©æ½°ï¼Œä½†æ­·å²è¨˜éŒ„ä¸æœƒä¿å­˜
            return ConversationBufferMemory(memory_key="chat_history", return_messages=True)
            
    return memory_store[session_id]

# ==================== LangChain Tools ====================

def search_knowledge_base(query: str) -> str:
    """æœå°‹çŸ¥è­˜åº«å·¥å…·"""
    logger.info(f"ğŸ› ï¸ ä½¿ç”¨å·¥å…· [æœå°‹çŸ¥è­˜åº«]ï¼ŒæŸ¥è©¢: {query}")
    try:
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        docs = retriever.invoke(query) # ä½¿ç”¨ invoke
        if docs:
            result = "\n\n".join([f"ä¾†æº {i+1} ({doc.metadata.get('source', 'æœªçŸ¥')[-30:]}):\n{doc.page_content}" for i, doc in enumerate(docs)])
            logger.info(f"âœ… å·¥å…· [æœå°‹çŸ¥è­˜åº«] æ‰¾åˆ° {len(docs)} ç­†è³‡æ–™")
            return f"æ‰¾åˆ°ç›¸é—œè³‡æ–™ï¼š\n{result}"
        else:
            logger.warning(f"âš ï¸ å·¥å…· [æœå°‹çŸ¥è­˜åº«] æœªæ‰¾åˆ°è³‡æ–™ for query: {query}")
            return "çŸ¥è­˜åº«ä¸­æ‰¾ä¸åˆ°èˆ‡æ­¤ç›´æ¥ç›¸é—œçš„è³‡æ–™ã€‚"
    except Exception as e:
        logger.error(f"âŒ å·¥å…· [æœå°‹çŸ¥è­˜åº«] åŸ·è¡ŒéŒ¯èª¤: {e}", exc_info=True)
        return f"æœå°‹çŸ¥è­˜åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"

def get_policy_info(policy_name: str) -> str:
    """å–å¾—ç‰¹å®šæ”¿ç­–è³‡è¨Šå·¥å…·"""
    logger.info(f"ğŸ› ï¸ ä½¿ç”¨å·¥å…· [æŸ¥è©¢æ”¿ç­–]ï¼Œæ”¿ç­–åç¨±: {policy_name}")
    try:
        # ä½¿ç”¨ similarity_search å¯èƒ½æ›´é©åˆç›´æ¥æ‰¾ç‰¹å®šåç¨±
        docs = vectorstore.similarity_search(policy_name, k=2)
        if docs:
            result = f"ä¾†æº ({docs[0].metadata.get('source', 'æœªçŸ¥')[-30:]}):\n{docs[0].page_content}"
            logger.info(f"âœ… å·¥å…· [æŸ¥è©¢æ”¿ç­–] æ‰¾åˆ°è³‡æ–™ for policy: {policy_name}")
            return f"é—œæ–¼ '{policy_name}' çš„è³‡è¨Šï¼š{result}"
        else:
            logger.warning(f"âš ï¸ å·¥å…· [æŸ¥è©¢æ”¿ç­–] æœªæ‰¾åˆ°è³‡æ–™ for policy: {policy_name}")
            return f"çŸ¥è­˜åº«ä¸­æ‰¾ä¸åˆ°åç‚º '{policy_name}' çš„ç‰¹å®šæ”¿ç­–è³‡è¨Šã€‚"
    except Exception as e:
        logger.error(f"âŒ å·¥å…· [æŸ¥è©¢æ”¿ç­–] åŸ·è¡ŒéŒ¯èª¤: {e}", exc_info=True)
        return f"æŸ¥è©¢æ”¿ç­– '{policy_name}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"

# å®šç¾© Agent å·¥å…·
tools = [
    Tool(
        name="æœå°‹çŸ¥è­˜åº«",
        func=search_knowledge_base,
        description="ç•¶ä½ éœ€è¦å›ç­”é—œæ–¼å¸‚é•·çš„**æ”¿ç­–ã€ç†å¿µã€æ–½æ”¿å ±å‘Šã€å…¬é–‹ç™¼è¨€ã€å€‹äººèƒŒæ™¯**æˆ–**æ¡ƒåœ’å¸‚æ”¿ç›¸é—œå•é¡Œ**æ™‚ä½¿ç”¨ã€‚**è¼¸å…¥ï¼š** å…·é«”çš„å•é¡Œæˆ–æ¸…æ™°çš„é—œéµå­—è©çµ„ (ä¾‹å¦‚ï¼š'æ¡ƒåœ’å¸‚çš„äº¤é€šæ”¿ç­–æœ‰å“ªäº›ï¼Ÿ', 'å¸‚é•·å°æ–¼é’å¹´å°±æ¥­çš„çœ‹æ³•', 'èªªæ˜ç¤¾æœƒä½å®…çš„é€²åº¦')ã€‚**ä¸è¦**åªè¼¸å…¥æ¨¡ç³Šçš„å–®è©ã€‚"
    ),
    Tool(
        name="æŸ¥è©¢ç‰¹å®šæ”¿ç­–åç¨±",
        func=get_policy_info,
        description="ç•¶ä½¿ç”¨è€…**æ˜ç¢º**æåˆ°ä¸€å€‹**å…·é«”çš„æ”¿ç­–åç¨±**ï¼Œè€Œä½ éœ€è¦æŸ¥æ‰¾è©²æ”¿ç­–çš„è©³ç´°å…§å®¹æ™‚ä½¿ç”¨ã€‚**è¼¸å…¥ï¼š** å®Œæ•´çš„æ”¿ç­–åç¨± (ä¾‹å¦‚ï¼š'äº”æ­²å¹¼å…’æ•™è‚²åŠ©å­¸é‡‘', 'åœ‹ä¸­å°å…è²»ç‡Ÿé¤Šåˆé¤')ã€‚å¦‚æœåªæ˜¯å•æŸå€‹é ˜åŸŸçš„æ”¿ç­– (å¦‚ 'äº¤é€šæ”¿ç­–')ï¼Œè«‹ä½¿ç”¨ã€Œæœå°‹çŸ¥è­˜åº«ã€ã€‚"
    ),
]

# ==================== LangChain Agent å®šç¾© ====================

# --- ä¿®æ­£ Agent Prompt ---
AGENT_PROMPT = """ä½ æ˜¯æ¡ƒåœ’å¸‚é•·å¼µå–„æ”¿çš„ AI åˆ†èº«ã€Œå–„å¯¶ã€ï¼Œä¸€å€‹è¦ªåˆ‡ã€å°ˆæ¥­ã€ç•¥å¸¶å¹½é»˜æ„Ÿçš„ AI åŠ©ç†ã€‚ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šæä¾›çš„å·¥å…·å’Œå°è©±è¨˜éŒ„ï¼Œä»¥å¸‚é•·çš„å£å»å›ç­”å¸‚æ°‘çš„å•é¡Œã€‚

**ä½ çš„å›ç­”é¢¨æ ¼ï¼š**
1.  **èªæ°£ï¼š** è¦ªåˆ‡ã€å°ˆæ¥­ã€æœ‰è€å¿ƒï¼Œå¶çˆ¾å¸¶é»è¼•é¬†å¹½é»˜ï¼Œå°±åƒå¸‚é•·æœ¬äººä¸€æ¨£ã€‚
2.  **é–‹é ­ï¼š** å¯ä»¥ç”¨ "å—¨ï¼"ã€"ä½ å¥½ï¼" æˆ– "å¸‚æ°‘æ‚¨å¥½" é–‹é ­ï¼Œä½†é¿å…æ¯æ¬¡éƒ½ä¸€æ¨£ã€‚
3.  **è‡ªç¨±ï¼š** ä½¿ç”¨ã€Œæˆ‘ã€æˆ–ã€Œå–„å¯¶ã€ã€‚
4.  **å…§å®¹ï¼š** å„ªå…ˆä½¿ç”¨å·¥å…·å¾çŸ¥è­˜åº«æŸ¥æ‰¾**æº–ç¢º**è³‡è¨Šã€‚å¦‚æœæ‰¾ä¸åˆ°ï¼Œ**èª å¯¦å‘ŠçŸ¥**æ‰¾ä¸åˆ°å…·é«”ç´°ç¯€ï¼Œä½†å¯ä»¥æä¾›ä¸€èˆ¬æ€§èªªæ˜æˆ–å»ºè­°æ´½è©¢å¸‚åºœã€‚
5.  **ç°¡æ½”ï¼š** å›ç­”è¦æŠ“ä½é‡é»ï¼Œé¿å…éæ–¼å†—é•·ã€‚
6.  **å®‰å…¨ï¼š** é¿é–‹æ•æ„Ÿæ”¿æ²»è©±é¡Œæˆ–äººèº«æ”»æ“Š (ä¾‹å¦‚é¸èˆ‰ã€æ‰¹è©•ç‰¹å®šäººç‰©)ã€‚å¦‚æœé‡åˆ°ï¼Œä½¿ç”¨å›ºå®šå›æ‡‰ï¼šâ€œé€™å€‹å•é¡Œæ¯”è¼ƒæ•æ„Ÿï¼Œå»ºè­°æ‚¨é—œæ³¨æ¡ƒåœ’å¸‚æ”¿åºœå®˜æ–¹ç¶²ç«™çš„æ­£å¼è³‡è¨Šï¼Œæˆ–ç›´æ¥æ’¥æ‰“ 1999 å¸‚æ°‘å°ˆç·šè©¢å•ã€‚æˆ‘å€‘å¾ˆæ¨‚æ„ç‚ºæ‚¨æœå‹™ï¼ğŸ˜Šâ€

**å¯ç”¨å·¥å…·ï¼š**
{tools}

**ä½¿ç”¨å·¥å…·çš„æ€è€ƒæµç¨‹ (ReAct æ ¼å¼)ï¼š**
Question: ä½¿ç”¨è€…æå‡ºçš„å•é¡Œã€‚
Thought: ä»”ç´°åˆ†æå•é¡Œã€‚åˆ¤æ–·æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·ï¼Ÿ
    * å¦‚æœæ˜¯**ç°¡å–®å•å€™** (ä¾‹å¦‚ï¼šä½ å¥½ã€ä½ æ˜¯èª°)ï¼Œæˆ–**å¸¸è­˜æ€§å•é¡Œ**ï¼Œæˆ–è€…æ¶‰åŠ**æ•æ„Ÿè©±é¡Œ**ï¼Œ**ä¸éœ€è¦**ä½¿ç”¨å·¥å…·ï¼Œå¯ä»¥ç›´æ¥æ€è€ƒ Final Answerã€‚
    * å¦‚æœæ˜¯é—œæ–¼**å¸‚æ”¿ã€æ”¿ç­–ã€å¸‚é•·ç†å¿µ/ç™¼è¨€/èƒŒæ™¯**çš„å•é¡Œï¼Œ**éœ€è¦**ä½¿ç”¨å·¥å…·ã€‚é¸æ“‡æœ€é©åˆçš„å·¥å…· (æœå°‹çŸ¥è­˜åº« æˆ– æŸ¥è©¢ç‰¹å®šæ”¿ç­–åç¨±)ã€‚æ±ºå®š Action Input (è¦æŸ¥è©¢çš„é—œéµå­—æˆ–æ”¿ç­–åç¨±)ã€‚
Action: é¸æ“‡çš„å·¥å…·åç¨± (ä¾‹å¦‚ï¼šæœå°‹çŸ¥è­˜åº«)ã€‚
Action Input: æä¾›çµ¦å·¥å…·çš„è¼¸å…¥ (ä¾‹å¦‚ï¼šæ¡ƒåœ’å¸‚ç¤¾æœƒä½å®…é€²åº¦)ã€‚
Observation: å·¥å…·è¿”å›çš„çµæœ (å¯èƒ½æ˜¯æ‰¾åˆ°çš„è³‡æ–™ï¼Œæˆ– "æ‰¾ä¸åˆ°è³‡æ–™" çš„è¨Šæ¯)ã€‚
Thought: æª¢è¦– Observationã€‚
    * å¦‚æœæ‰¾åˆ°æ»¿æ„çš„è³‡æ–™ï¼Œæ ¹æ“šè³‡æ–™å’Œå°è©±è¨˜éŒ„ï¼Œä»¥å¸‚é•·å£å»çµ„ç¹” Final Answerã€‚
    * å¦‚æœå·¥å…·è¿”å› "æ‰¾ä¸åˆ°è³‡æ–™" æˆ–è³‡æ–™ä¸ç›¸é—œï¼Œ**ä¸è¦**å†æ¬¡å˜—è©¦**ç›¸åŒ**çš„æŸ¥è©¢ã€‚æ€è€ƒæ˜¯å¦å¯ä»¥ç”¨**ä¸åŒ**çš„é—œéµå­—å†è©¦ä¸€æ¬¡ã€Œæœå°‹çŸ¥è­˜åº«ã€(æœ€å¤šå˜—è©¦ 1-2 æ¬¡ä¸åŒçš„é—œéµå­—)ã€‚å¦‚æœé‚„æ˜¯æ‰¾ä¸åˆ°ï¼Œå°±åœ¨ Final Answer ä¸­**èª å¯¦èªªæ˜**æ‰¾ä¸åˆ°å…·é«”ç´°ç¯€ï¼Œä¸¦æä¾›ä¸€èˆ¬æ€§å»ºè­°ã€‚**çµ•å°ä¸è¦**å¹»æƒ³æˆ–ç·¨é€ ç­”æ¡ˆã€‚
    * å¦‚æœå·¥å…·è¿”å›éŒ¯èª¤è¨Šæ¯ï¼Œä¹Ÿåœ¨ Final Answer ä¸­èªªæ˜æŸ¥è©¢æ™‚é‡åˆ°å•é¡Œã€‚
Final Answer: [**é€™è£¡ç›´æ¥å¯«å‡º**ä½ æœ€çµ‚è¦çµ¦ä½¿ç”¨è€…çš„å®Œæ•´å›è¦†ï¼Œç”¨å¸‚é•·çš„å£å»ï¼Œ**ä¸è¦åŒ…å«** "Final Answer:" é€™å€‹æ¨™ç±¤æœ¬èº«ã€‚]

**é‡è¦ï¼š** å³ä½¿ä½ çŸ¥é“ç­”æ¡ˆï¼Œå¦‚æœå•é¡Œæ¶‰åŠå¸‚æ”¿æˆ–æ”¿ç­–ç´°ç¯€ï¼Œ**ä¹Ÿæ‡‰è©²å„ªå…ˆä½¿ç”¨å·¥å…·**ç¢ºèªè³‡è¨Šçš„æº–ç¢ºæ€§ã€‚é™¤éæ˜¯åƒã€Œä½ æ˜¯èª°ã€é€™ç¨®åŸºæœ¬è‡ªæˆ‘ä»‹ç´¹ã€‚

**å°è©±è¨˜éŒ„ (æœ€è¿‘çš„å°è©±)ï¼š**
{chat_history}

**é–‹å§‹ï¼**

Question: {input}
Thought: {agent_scratchpad}"""

agent_prompt = PromptTemplate(
    template=AGENT_PROMPT,
    # --- ä¿®æ­£ï¼šç§»é™¤ 'tool_names' ---
    input_variables=["input", "chat_history", "agent_scratchpad", "tools"]
)

# å‰µå»º Agent
try:
    agent = create_react_agent(
        llm=llm,
        tools=tools,
        prompt=agent_prompt
    )
    logger.info("âœ… ReAct Agent å‰µå»ºæˆåŠŸ")
except Exception as agent_create_err:
    logger.error(f"âŒ å‰µå»º Agent å¤±æ•—: {agent_create_err}", exc_info=True)
    agent = None # æ¨™è¨˜ Agent å‰µå»ºå¤±æ•—

# ==================== Pydantic æ¨¡å‹ ====================
# (ä¿æŒä¸è®Š)
class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = "default"
    use_agent: bool = True

class ChatResponse(BaseModel):
    reply: str
    sources: Optional[List[str]] = []
    session_id: str
    timestamp: str
    thought_process: Optional[str] = None # æ”¹ç‚ºå­—ä¸²ä»¥å®¹ç´éŒ¯èª¤è¨Šæ¯æˆ–æ­¥é©Ÿ

class ContentGenerationRequest(BaseModel):
    topic: str
    style: str = "æ­£å¼"
    length: str = "ä¸­"
    context: Optional[str] = None

class IngestRequest(BaseModel):
    folder_path: str = "documents"

# ==================== å·¥å…·å‡½æ•¸ ====================
# (ä¿æŒä¸è®Š)
def verify_admin(authorization: Optional[str] = Header(None)):
    """é©—è­‰ç®¡ç†å“¡æ¬Šé™"""
    # å¯¦éš›æ‡‰ç”¨æ‡‰ä½¿ç”¨æ›´å®‰å…¨çš„é©—è­‰æ–¹å¼
    if not ADMIN_PASSWORD or authorization != f"Bearer {ADMIN_PASSWORD}":
        logger.warning(f"ğŸš« ç®¡ç†å“¡æ¬Šé™é©—è­‰å¤±æ•—: {authorization}")
        raise HTTPException(status_code=401, detail="æœªæˆæ¬Šæˆ–æœªè¨­å®šç®¡ç†å“¡å¯†ç¢¼")
    return True

def load_document(file_path: str):
    """è¼‰å…¥æ–‡ä»¶"""
    file_extension = Path(file_path).suffix.lower()
    loader = None
    try:
        if file_extension == ".pdf":
            loader = PyPDFLoader(file_path)
        elif file_extension in [".docx", ".doc"]:
            # ç¢ºä¿å·²å®‰è£ python-docx å’Œ docx2txt
            try:
                import docx2txt # æª¢æŸ¥æ˜¯å¦å®‰è£
                loader = Docx2txtLoader(file_path)
            except ImportError:
                logger.error("âŒ ç¼ºå°‘ 'docx2txt' æ¨¡çµ„ï¼Œç„¡æ³•è®€å– .docx æ–‡ä»¶ã€‚è«‹åŸ·è¡Œ 'pip install docx2txt'")
                return []
        elif file_extension == ".txt":
            # å˜—è©¦ç”¨ utf-8 é–‹å•Ÿï¼Œå¤±æ•—å‰‡å˜—è©¦ç³»çµ±é è¨­ç·¨ç¢¼
            try:
                loader = TextLoader(file_path, encoding="utf-8")
                # å˜—è©¦è®€å–ä¸€å°éƒ¨åˆ†ä»¥è§¸ç™¼å¯èƒ½çš„è§£ç¢¼éŒ¯èª¤
                loader.load()[0].page_content[:10]
            except UnicodeDecodeError:
                logger.warning(f"âš ï¸ ä½¿ç”¨ UTF-8 è®€å– {file_path} å¤±æ•—ï¼Œå˜—è©¦ç³»çµ±é è¨­ç·¨ç¢¼...")
                loader = TextLoader(file_path) # ä½¿ç”¨ç³»çµ±é è¨­
            except Exception as txt_err: # æ•æ‰å…¶ä»–å¯èƒ½çš„è®€å–éŒ¯èª¤
                 logger.error(f"âŒ è®€å– TXT æ–‡ä»¶ {file_path} æ™‚ç™¼ç”Ÿéé æœŸçš„éŒ¯èª¤: {txt_err}")
                 return []

        else:
            logger.warning(f"âš ï¸ ä¸æ”¯æ´çš„æª”æ¡ˆé¡å‹: {file_extension} ({file_path})")
            return []

        if loader:
            documents = loader.load()
            logger.info(f"âœ… æˆåŠŸè¼‰å…¥: {file_path} ({len(documents)} æ®µ)")
            return documents
        else:
            return []
    except Exception as e:
        logger.error(f"âŒ è¼‰å…¥æ–‡ä»¶ {file_path} å¤±æ•—: {e}", exc_info=True)
        return []

# ==================== Prompt æ¨¡æ¿ ====================
# (ä¿æŒä¸è®Š)
RAG_PROMPT = PromptTemplate(
    template="""ä½ æ˜¯å¸‚é•·çš„ AI åŠ©ç†ï¼Œè«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™å›ç­”å•é¡Œã€‚

ç›¸é—œè³‡æ–™ï¼š
{context}

å°è©±è¨˜éŒ„ï¼š
{chat_history}

å•é¡Œï¼š{question}

å›ç­”æ™‚è«‹ï¼š
- ä½¿ç”¨å¸‚é•·è¦ªåˆ‡ã€å°ˆæ¥­çš„èªæ°£
- å¦‚æœè³‡æ–™ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹èª å¯¦å‘ŠçŸ¥
- ä¿æŒç°¡æ½”æ˜ç­
- é©æ™‚ä½¿ç”¨è¼•é¬†çš„èªæ°£

å›ç­”ï¼š""",
    input_variables=["context", "chat_history", "question"]
)

CONTENT_PROMPT = PromptTemplate(
    template="""ä½ æ˜¯å¸‚é•·çš„å°ˆå±¬æ–‡æ¡ˆç”ŸæˆåŠ©ç†ã€‚

ä»»å‹™ï¼šç”Ÿæˆ {style} é¢¨æ ¼ã€{length} ç¯‡å¹…çš„æ–‡æ¡ˆ

ä¸»é¡Œï¼š{topic}

åƒè€ƒè³‡æ–™ï¼š
{context}

è¦æ±‚ï¼š
- å®Œå…¨æ¨¡ä»¿å¸‚é•·çš„èªæ°£å’Œç”¨è©ç¿’æ…£
- é¢¨æ ¼ï¼š{style}ï¼ˆæ­£å¼/è¼•é¬†/å¹½é»˜ï¼‰
- ç¯‡å¹…ï¼š{length}ï¼ˆçŸ­=50-100å­—ï¼Œä¸­=150-300å­—ï¼Œé•·=400-600å­—ï¼‰
- å…§å®¹å¿…é ˆåŸºæ–¼çŸ¥è­˜åº«è³‡è¨Š
- é¿å…è™›å‡å…§å®¹

æ–‡æ¡ˆå…§å®¹ï¼š""",
    input_variables=["topic", "style", "length", "context"]
)

# ==================== API ç«¯é» ====================

@app.get("/")
async def root():
    # ... (ä¿æŒä¸è®Š)
    return {
        "system": "PAIS æ”¿å‹™åˆ†èº«æ™ºèƒ½ç³»çµ±",
        "version": app.version,
        "framework": "LangChain (Agents + Memory + RAG + Tools)",
        "status": "ğŸŸ¢ é‹è¡Œä¸­"
    }

@app.get("/health")
async def health_check():
    # ... (ä¿æŒä¸è®Š)
    qdrant_ok = False
    llm_ok = False
    agent_ok = agent is not None
    error_msg = ""
    try:
        qdrant_client.get_collections()
        qdrant_ok = True
    except Exception as e:
        error_msg += f"Qdrant é€£æ¥å¤±æ•—: {e}; "
        logger.error(f"âŒ å¥åº·æª¢æŸ¥ - Qdrant é€£æ¥å¤±æ•—: {e}")

    # æš«æ™‚å‡è¨­ LLM æ­£å¸¸ï¼Œé¿å… API Key æ¶ˆè€—
    llm_ok = True


    status = "healthy" if qdrant_ok and llm_ok and agent_ok else "unhealthy"

    return {
        "status": status,
        "qdrant": "âœ… connected" if qdrant_ok else "âŒ disconnected",
        "llm": "âœ… assumed ok" if llm_ok else "âŒ failed test",
        "agents": "âœ… active" if agent_ok else "âŒ failed to create",
        "error": error_msg if error_msg else None
    }

@app.post("/api/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    å°è©± API (LangChain Agent + Memory æˆ– RAG Chain)
    """
    reply: str = "å“å‘€ï¼Œå–„å¯¶å¥½åƒæœ‰é»ç´¯äº†ï¼Œæˆ–æ˜¯ç¶²è·¯ä¸å¤ªç©©å®šï¼Œè«‹ç¨å¾Œå†è©¦ä¸€æ¬¡å–”ï¼"
    sources: List[str] = []
    thought_process_str: Optional[str] = None
    result: Optional[Dict[str, Any]] = None

    session_id = request.session_id or "default"

    try:
        logger.info(f"ğŸ’¬ [{session_id}] æ”¶åˆ°å•é¡Œ: {request.message}")

        if not agent and request.use_agent:
             logger.error(f"âŒ Agent æœªæˆåŠŸåˆå§‹åŒ–ï¼Œç„¡æ³•è™•ç† Agent è«‹æ±‚ ({session_id})")
             raise HTTPException(status_code=500, detail="ç³»çµ± Agent å…ƒä»¶å•Ÿå‹•å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")

        memory = get_memory(session_id)

        if request.use_agent:
            # --- ç‚º Agent å‹•æ…‹è¨­å®š output_key ---
            memory.output_key = "output"

            agent_executor = AgentExecutor(
                agent=agent,
                tools=tools,
                memory=memory,
                verbose=True, # ä¿ç•™è©³ç´°æ—¥èªŒä»¥ä¾›é™¤éŒ¯
                # --- å¢åŠ  Agent åŸ·è¡Œæ¬¡æ•¸ ---
                max_iterations=5, # å¾ 3 å¢åŠ åˆ° 5
                handle_parsing_errors=True, # ç¹¼çºŒè™•ç†å¯èƒ½çš„ LLM æ ¼å¼éŒ¯èª¤
            )

            logger.info(f"ğŸš€ [{session_id}] é–‹å§‹åŸ·è¡Œ Agent...")
            result = agent_executor.invoke({"input": request.message})
            logger.info(f"âœ… [{session_id}] Agent åŸ·è¡Œå®Œæˆ")

            # Agent çš„å›è¦†å›ºå®šåœ¨ 'output'
            reply = result.get("output", reply) # ä½¿ç”¨ get ä¸¦æä¾›é è¨­å€¼

            # å˜—è©¦æå–æ€è€ƒéç¨‹ (æ³¨æ„ï¼šintermediate_steps å¯èƒ½å¾ˆé•·)
            intermediate_steps = result.get("intermediate_steps")
            if intermediate_steps:
                 # è½‰æ›ç‚ºè¼ƒæ˜“è®€çš„å­—ä¸²æ ¼å¼ï¼Œåªå–éƒ¨åˆ†é—œéµè³‡è¨Š
                 try:
                     thought_process_str = "\n---\n".join([
                         f"Thought: {step[0].log.strip()}\nAction: {step[0].tool}({step[0].tool_input})\nObservation: {str(step[1])[:200]}..." # é™åˆ¶ Observation é•·åº¦
                         for step in intermediate_steps
                     ])
                 except Exception as fmt_err:
                     logger.warning(f"âš ï¸ ç„¡æ³•æ ¼å¼åŒ– intermediate_steps: {fmt_err}")
                     thought_process_str = str(intermediate_steps)[:1000] + "..." # æˆªæ–·åŸå§‹å­—ä¸²
            else:
                 thought_process_str = "Agent æœªä½¿ç”¨å·¥å…·æˆ–ç„¡ä¸­é–“æ­¥é©Ÿè¨˜éŒ„ã€‚"

            # Agent ç›®å‰ä¸ç›´æ¥å›å‚³ sourcesï¼Œä½†å¯ä»¥å¾æ€è€ƒéç¨‹ä¸­æå–
            sources = [] # æš«æ™‚ä¸è™•ç†

        else: # ä½¿ç”¨ RAG Chain
            # --- ç‚º RAG éˆå‹•æ…‹è¨­å®š output_key ---
            memory.output_key = "answer"

            qa_chain = ConversationalRetrievalChain.from_llm(
                llm=llm,
                retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
                memory=memory,
                combine_docs_chain_kwargs={"prompt": RAG_PROMPT},
                return_source_documents=True,
                verbose=True # RAG Chain ä¹Ÿé–‹å•Ÿè©³ç´°æ—¥èªŒ
            )

            logger.info(f"ğŸš€ [{session_id}] é–‹å§‹åŸ·è¡Œ RAG Chain...")
            result = qa_chain.invoke({"question": request.message})
            logger.info(f"âœ… [{session_id}] RAG Chain åŸ·è¡Œå®Œæˆ")

            reply = result.get("answer", reply) # RAG chain çš„å›è¦†åœ¨ 'answer'
            thought_process_str = "ä½¿ç”¨ RAG Chain æ¨¡å¼ï¼Œç„¡ ReAct æ€è€ƒéç¨‹ã€‚"
            sources = [
                doc.metadata.get("source", "æœªçŸ¥ä¾†æº").split('/')[-1] # åªå–æª”å
                for doc in result.get("source_documents", [])
            ]

        logger.info(f"ğŸ¤– [{session_id}] æœ€çµ‚å›è¦† (å‰100å­—): {reply[:100]}...")

        return ChatResponse(
            reply=reply,
            sources=list(set(sources)), # å»é‡
            session_id=session_id,
            timestamp=datetime.now().isoformat(),
            thought_process=thought_process_str # å›å‚³æ€è€ƒéç¨‹å­—ä¸²
        )

    except Exception as e:
        # å¢åŠ  exc_info=True ä¾†ç²å–æ›´è©³ç´°çš„éŒ¯èª¤å †ç–Š
        logger.error(f"âŒ å°è©±è™•ç†å¤±æ•— ({session_id}): {e}", exc_info=True)

        # éŒ¯èª¤æ™‚ä¹Ÿå˜—è©¦è¨˜éŒ„æœ€å¾Œçš„ result (å¦‚æœæœ‰çš„è©±)
        error_thought_process = f"éŒ¯èª¤: {str(e)}"
        if result:
            error_thought_process += f"\næœ€å¾Œçš„ Agent/Chain çµæœ: {str(result)[:500]}..."

        # 'reply' å·²ç¶“æœ‰é è¨­çš„éŒ¯èª¤è¨Šæ¯äº†
        return ChatResponse(
            reply=reply, # å›å‚³é è¨­çš„éŒ¯èª¤è¨Šæ¯
            sources=[],
            session_id=session_id,
            timestamp=datetime.now().isoformat(),
            thought_process=error_thought_process # å›å‚³éŒ¯èª¤è©³æƒ…
        )

# --- /api/generate ä¿æŒä¸è®Š ---
@app.post("/api/generate")
async def generate_content(
    request: ContentGenerationRequest,
    admin: bool = Depends(verify_admin)
):
    """æ–‡æ¡ˆç”Ÿæˆ API"""
    try:
        logger.info(f"âœï¸ é–‹å§‹ç”Ÿæˆæ–‡æ¡ˆ: ä¸»é¡Œ='{request.topic}', é¢¨æ ¼='{request.style}', é•·åº¦='{request.length}'")

        context = "ï¼ˆç„¡ç‰¹åˆ¥æŒ‡å®šçš„åƒè€ƒè³‡æ–™ï¼‰"
        relevant_docs = []
        if request.context: # å¦‚æœå‰ç«¯æœ‰æŒ‡å®šç‰¹å®šä¾†æºï¼ˆæœªä¾†å¯æ“´å……ï¼‰
             logger.info(f"ğŸ” ä½¿ç”¨æŒ‡å®š context é€²è¡Œç”Ÿæˆ")
             context = request.context
        else: # é è¨­å¾å‘é‡åº«æ‰¾ç›¸é—œè³‡æ–™
            try:
                retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
                logger.info(f"ğŸ” å¾çŸ¥è­˜åº«æœå°‹ä¸»é¡Œ '{request.topic}' çš„åƒè€ƒè³‡æ–™...")
                relevant_docs = retriever.invoke(request.topic) # ä½¿ç”¨ invoke
                if relevant_docs:
                    context = "\n\n---\n\n".join([doc.page_content for doc in relevant_docs])
                    logger.info(f"âœ… æ‰¾åˆ° {len(relevant_docs)} ç­†åƒè€ƒè³‡æ–™")
                else:
                    logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ä¸»é¡Œ '{request.topic}' çš„ç›¸é—œåƒè€ƒè³‡æ–™")
            except Exception as search_err:
                 logger.error(f"âŒ æœå°‹åƒè€ƒè³‡æ–™æ™‚å¤±æ•—: {search_err}")

        content_chain = LLMChain(
            llm=llm,
            prompt=CONTENT_PROMPT,
            verbose=True
        )

        logger.info(f"ğŸš€ é–‹å§‹èª¿ç”¨ LLM ç”Ÿæˆæ–‡æ¡ˆ...")
        result = content_chain.invoke({
            "topic": request.topic,
            "style": request.style,
            "length": request.length,
            "context": context
        })

        generated_content = result.get("text", "").strip()

        if not generated_content:
             logger.error("âŒ LLM è¿”å›äº†ç©ºçš„æ–‡æ¡ˆå…§å®¹")
             raise HTTPException(status_code=500, detail="æ–‡æ¡ˆç”Ÿæˆå¤±æ•—ï¼šæ¨¡å‹æœªè¿”å›æœ‰æ•ˆå…§å®¹")

        logger.info(f"âœ… æ–‡æ¡ˆç”ŸæˆæˆåŠŸ (é•·åº¦: {len(generated_content)})")

        # --- è‡ªå‹•å„²å­˜ç”Ÿæˆçš„æ–‡æ¡ˆ ---
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # æ¸…ç† topic ä½œç‚ºæª”åä¸€éƒ¨åˆ†
            safe_topic = "".join(c if c.isalnum() else "_" for c in request.topic[:20])
            output_file = Path("generated_content") / f"{timestamp}_{safe_topic}.txt"
            output_file.parent.mkdir(exist_ok=True) # ç¢ºä¿ç›®éŒ„å­˜åœ¨

            with open(output_file, "w", encoding="utf-8") as f:
                f.write(f"ä¸»é¡Œ: {request.topic}\n")
                f.write(f"é¢¨æ ¼: {request.style}\n")
                f.write(f"ç¯‡å¹…: {request.length}\n")
                f.write(f"æ™‚é–“: {datetime.now().isoformat()}\n")
                f.write("=" * 50 + "\n\n")
                f.write(generated_content)
            logger.info(f"ğŸ’¾ æ–‡æ¡ˆå·²è‡ªå‹•å„²å­˜è‡³: {output_file}")
            file_path_str = str(output_file)
        except Exception as save_err:
            logger.error(f"âŒ è‡ªå‹•å„²å­˜æ–‡æ¡ˆå¤±æ•—: {save_err}")
            file_path_str = None # å„²å­˜å¤±æ•—

        return {
            "content": generated_content,
            "file_path": file_path_str, # å›å‚³å„²å­˜è·¯å¾‘
            "sources": [
                 doc.metadata.get("source", "æœªçŸ¥ä¾†æº").split('/')[-1] # åªå–æª”å
                 for doc in relevant_docs # ä½¿ç”¨å‰é¢æœå°‹åˆ°çš„ docs
            ],
            "context_used": len(relevant_docs) > 0 # æ˜¯å¦ä½¿ç”¨äº†çŸ¥è­˜åº« context
        }

    except HTTPException as http_exc:
        raise http_exc # æŠŠ HTTP éŒ¯èª¤ç›´æ¥æ‹‹å‡º
    except Exception as e:
        logger.error(f"âŒ æ–‡æ¡ˆç”Ÿæˆéç¨‹ä¸­ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æ–‡æ¡ˆç”Ÿæˆå¤±æ•—: {str(e)}")


# --- /api/ingest, /api/upload ä¿æŒä¸è®Š ---
@app.post("/api/ingest")
async def ingest_documents(
    request: IngestRequest,
    admin: bool = Depends(verify_admin)
):
    """çŸ¥è­˜åº«ä¸Šå‚³ API (è™•ç†æŒ‡å®šè³‡æ–™å¤¾å…§æ‰€æœ‰æ”¯æ´æ–‡ä»¶)"""
    processed_files_count = 0
    total_chunks_created = 0
    errors = []

    try:
        folder_path = Path(request.folder_path)
        if not folder_path.exists() or not folder_path.is_dir():
            logger.error(f"âŒ Ingest - è³‡æ–™å¤¾ä¸å­˜åœ¨æˆ–ä¸æ˜¯æœ‰æ•ˆç›®éŒ„: {folder_path}")
            raise HTTPException(status_code=404, detail=f"è³‡æ–™å¤¾ '{folder_path}' ä¸å­˜åœ¨")

        supported_extensions = [".pdf", ".docx", ".doc", ".txt"]
        logger.info(f"ğŸ“š é–‹å§‹è™•ç†è³‡æ–™å¤¾: {folder_path}")

        # ä½¿ç”¨ rglob éè¿´æœå°‹
        files_to_process = [f for f in folder_path.rglob("*")
                            if f.is_file() and f.suffix.lower() in supported_extensions]

        if not files_to_process:
            logger.warning(f"âš ï¸ è³‡æ–™å¤¾ '{folder_path}' ä¸­æ²’æœ‰æ‰¾åˆ°æ”¯æ´çš„æª”æ¡ˆ ({supported_extensions})")
            return {"message": "è³‡æ–™å¤¾ä¸­æ²’æœ‰æ‰¾åˆ°æ”¯æ´çš„æª”æ¡ˆ", "processed": 0, "chunks_created": 0}

        logger.info(f"ğŸ” æ‰¾åˆ° {len(files_to_process)} å€‹æ”¯æ´çš„æª”æ¡ˆï¼Œé–‹å§‹è¼‰å…¥èˆ‡åˆ†å‰²...")

        all_splits = []
        for file_path in files_to_process:
            logger.debug(f"â³ è™•ç†æª”æ¡ˆ: {file_path}")
            docs = load_document(str(file_path))
            if docs:
                for doc in docs:
                    # æ¨™æº–åŒ– source è·¯å¾‘
                    relative_path = file_path.relative_to(Path.cwd()) # ç›¸å°æ–¼ç›®å‰å·¥ä½œç›®éŒ„çš„è·¯å¾‘
                    doc.metadata["source"] = str(relative_path).replace("\\", "/") # çµ±ä¸€ä½¿ç”¨ /
                    doc.metadata["uploaded_at"] = datetime.now().isoformat()
                    doc.metadata["filename"] = file_path.name

                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000,
                    chunk_overlap=200,
                    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", "ã€", " ", ""], # åŠ å…¥ä¸­æ–‡æ¨™é»
                    length_function=len,
                    # add_start_index = True # å¯é¸ï¼Œå¢åŠ èµ·å§‹ç´¢å¼•å…ƒæ•¸æ“š
                )
                splits = text_splitter.split_documents(docs)
                logger.info(f"ğŸ“„ æª”æ¡ˆ {file_path.name} åˆ†å‰²æˆ {len(splits)} å€‹ç‰‡æ®µ")
                all_splits.extend(splits)
                processed_files_count += 1
            else:
                 logger.warning(f"âš ï¸ æª”æ¡ˆ {file_path} è¼‰å…¥å¤±æ•—æˆ–ç„¡å…§å®¹ï¼Œå·²è·³é")
                 errors.append(f"ç„¡æ³•è™•ç†æª”æ¡ˆ: {file_path.name}")


        if not all_splits:
            logger.error("âŒ ç„¡æ³•å¾ä»»ä½•æª”æ¡ˆä¸­æˆåŠŸåˆ†å‰²å‡ºç‰‡æ®µ")
            return {"message": "ç„¡æ³•æˆåŠŸè™•ç†ä»»ä½•æ–‡ä»¶å…§å®¹", "processed": processed_files_count, "chunks_created": 0, "errors": errors}

        total_chunks_created = len(all_splits)
        logger.info(f"âœ… æ–‡ä»¶åˆ†å‰²å®Œæˆ: å…± {processed_files_count} å€‹æª”æ¡ˆ, {total_chunks_created} å€‹ç‰‡æ®µã€‚æº–å‚™å¯«å…¥å‘é‡è³‡æ–™åº«...")

        # åˆ†æ‰¹å¯«å…¥ Qdrantï¼Œé¿å…ä¸€æ¬¡å‚³è¼¸éå¤§ payload
        batch_size = 100
        for i in range(0, total_chunks_created, batch_size):
            batch = all_splits[i:i + batch_size]
            try:
                vectorstore.add_documents(batch)
                logger.info(f"âœï¸ å·²å¯«å…¥ {len(batch)} å€‹ç‰‡æ®µ (ç¸½é€²åº¦: {min(i + batch_size, total_chunks_created)} / {total_chunks_created})")
            except Exception as add_doc_err:
                 logger.error(f"âŒ å¯«å…¥ç‰‡æ®µ {i} åˆ° {i+batch_size} æ™‚å¤±æ•—: {add_doc_err}", exc_info=True)
                 errors.append(f"éƒ¨åˆ†è³‡æ–™å¯«å…¥å¤±æ•—: {add_doc_err}")
                 # raise HTTPException(status_code=500, detail=f"éƒ¨åˆ†è³‡æ–™å¯«å…¥å‘é‡è³‡æ–™åº«å¤±æ•—: {add_doc_err}") # ä¸­æ–·

        logger.info(f"âœ… æ‰€æœ‰ç‰‡æ®µå·²æˆåŠŸå¯«å…¥å‘é‡è³‡æ–™åº« '{COLLECTION_NAME}'")

        return {
            "message": "âœ… çŸ¥è­˜åº«æ›´æ–°æˆåŠŸ" + (f" (éƒ¨åˆ†æª”æ¡ˆè™•ç†å¤±æ•—ï¼Œè«‹æŸ¥çœ‹æ—¥èªŒ)" if errors else ""),
            "files_processed": processed_files_count,
            "chunks_created": total_chunks_created,
            "collection": COLLECTION_NAME,
            "errors": errors if errors else None
        }

    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.error(f"âŒ çŸ¥è­˜åº«ä¸Šå‚³éç¨‹ä¸­ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"çŸ¥è­˜åº«è™•ç†å¤±æ•—: {str(e)}")

@app.post("/api/upload")
async def upload_file(
    file: UploadFile = File(...),
    admin: bool = Depends(verify_admin)
):
    """å–®å€‹æª”æ¡ˆä¸Šå‚³ä¸¦ç›´æ¥åŠ å…¥çŸ¥è­˜åº«"""
    try:
        # --- å„²å­˜ä¸Šå‚³çš„æª”æ¡ˆ ---
        upload_folder = Path("documents")
        upload_folder.mkdir(exist_ok=True)
        file_path = upload_folder / Path(file.filename).name

        # æª¢æŸ¥æª”åè¡çªï¼Œå¦‚æœå­˜åœ¨å‰‡åŠ ä¸Š timestamp
        if file_path.exists():
             timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
             file_path = upload_folder / f"{file_path.stem}_{timestamp}{file_path.suffix}"

        logger.info(f"ğŸ“¤ æ¥æ”¶åˆ°æª”æ¡ˆä¸Šå‚³: {file.filename}, å„²å­˜è‡³: {file_path}")

        try:
            with open(file_path, "wb") as buffer:
                content = await file.read()
                buffer.write(content)
            logger.info(f"ğŸ’¾ æª”æ¡ˆå„²å­˜æˆåŠŸ: {file_path}")
        except Exception as save_err:
             logger.error(f"âŒ å„²å­˜ä¸Šå‚³æª”æ¡ˆå¤±æ•— ({file.filename}): {save_err}", exc_info=True)
             raise HTTPException(status_code=500, detail=f"å„²å­˜æª”æ¡ˆå¤±æ•—: {save_err}")

        # --- è™•ç†æª”æ¡ˆä¸¦åŠ å…¥çŸ¥è­˜åº« (é¡ä¼¼ ingest) ---
        logger.info(f"ğŸ“š é–‹å§‹è™•ç†å–®ä¸€æª”æ¡ˆ: {file_path}")
        docs = load_document(str(file_path))

        if not docs:
            logger.warning(f"âš ï¸ æª”æ¡ˆ {file_path} è¼‰å…¥å¤±æ•—æˆ–ç„¡å…§å®¹ï¼Œç„¡æ³•åŠ å…¥çŸ¥è­˜åº«")
            return {
                "message": "æª”æ¡ˆå·²æˆåŠŸä¸Šå‚³ï¼Œä½†ç„¡æ³•è®€å–å…§å®¹æˆ–å…§å®¹ç‚ºç©ºï¼ŒæœªåŠ å…¥çŸ¥è­˜åº«ã€‚",
                "filename": file.filename,
                "chunks": 0,
                "error": "Failed to load or empty document"
            }

        for doc in docs:
            relative_path = file_path.relative_to(Path.cwd())
            doc.metadata["source"] = str(relative_path).replace("\\", "/")
            doc.metadata["uploaded_at"] = datetime.now().isoformat()
            doc.metadata["filename"] = file_path.name

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", "ã€", " ", ""],
            length_function=len
        )
        splits = text_splitter.split_documents(docs)
        total_chunks = len(splits)
        logger.info(f"ğŸ“„ æª”æ¡ˆ {file_path.name} åˆ†å‰²æˆ {total_chunks} å€‹ç‰‡æ®µ")

        if not splits:
             logger.warning(f"âš ï¸ æª”æ¡ˆ {file_path.name} åˆ†å‰²å¾Œç„¡ç‰‡æ®µï¼Œç„¡æ³•åŠ å…¥çŸ¥è­˜åº«")
             return {
                "message": "æª”æ¡ˆå·²æˆåŠŸä¸Šå‚³ï¼Œä½†åˆ†å‰²å¾Œç„¡æœ‰æ•ˆå…§å®¹ï¼ŒæœªåŠ å…¥çŸ¥è­˜åº«ã€‚",
                "filename": file.filename,
                "chunks": 0,
                "error": "No chunks generated after splitting"
            }

        try:
            vectorstore.add_documents(splits)
            logger.info(f"âœ… æª”æ¡ˆ {file_path.name} çš„ç‰‡æ®µå·²æˆåŠŸåŠ å…¥å‘é‡è³‡æ–™åº«")
            return {
                "message": "âœ… æª”æ¡ˆä¸Šå‚³ä¸¦æˆåŠŸåŠ å…¥çŸ¥è­˜åº«",
                "filename": file.filename,
                "chunks": total_chunks
            }
        except Exception as add_doc_err:
            logger.error(f"âŒ å°‡æª”æ¡ˆ {file_path.name} åŠ å…¥å‘é‡è³‡æ–™åº«æ™‚å¤±æ•—: {add_doc_err}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"åŠ å…¥çŸ¥è­˜åº«å¤±æ•—: {add_doc_err}")

    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.error(f"âŒ è™•ç†æª”æ¡ˆä¸Šå‚³æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤ ({file.filename}): {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æª”æ¡ˆä¸Šå‚³è™•ç†å¤±æ•—: {str(e)}")


# --- å…¶ä»– API ä¿æŒä¸è®Š ---
@app.get("/api/memory/{session_id}")
async def get_memory_history(session_id: str):
    """å–å¾—æŒ‡å®š session çš„å°è©±è¨˜æ†¶"""
    try:
        if session_id not in memory_store:
            history_file = Path(f"chat_history/{session_id}.json")
            if not history_file.exists():
                logger.warning(f"âš ï¸ è«‹æ±‚è¨˜æ†¶é«”æ­·å²ï¼Œä½† session '{session_id}' ä¸å­˜åœ¨ (è¨˜æ†¶é«”èˆ‡æª”æ¡ˆçš†ç„¡)")
                raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°æ­¤å°è©±è¨˜éŒ„")
            else:
                 get_memory(session_id)

        memory = memory_store[session_id]
        history_data = memory.load_memory_variables({})
        messages = history_data.get("chat_history", [])
        formatted_history = []
        for msg in messages:
            msg_type = "user" if msg.type == "human" else "ai" if msg.type == "ai" else "system"
            formatted_history.append({
                "type": msg_type,
                "content": msg.content
            })

        logger.info(f"âœ… æˆåŠŸå–å¾— session '{session_id}' çš„å°è©±æ­·å² ({len(formatted_history)} æ¢)")
        return {
            "session_id": session_id,
            "history": formatted_history
        }
    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.error(f"âŒ å–å¾—å°è©±æ­·å²å¤±æ•— ({session_id}): {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç„¡æ³•å–å¾—å°è©±è¨˜éŒ„: {str(e)}")

@app.delete("/api/memory/{session_id}")
async def clear_memory(session_id: str, admin: bool = Depends(verify_admin)):
    """æ¸…é™¤æŒ‡å®š session çš„å°è©±è¨˜æ†¶ (è¨˜æ†¶é«”èˆ‡æª”æ¡ˆ)"""
    try:
        deleted_from_memory = False
        deleted_from_file = False

        if session_id in memory_store:
            try:
                memory_store[session_id].clear()
                del memory_store[session_id]
                deleted_from_memory = True
                logger.info(f"ğŸ—‘ï¸ å·²å¾è¨˜æ†¶é«”ä¸­æ¸…é™¤ session: {session_id}")
            except Exception as mem_clear_err:
                 logger.error(f"âŒ æ¸…é™¤è¨˜æ†¶é«”ä¸­ session '{session_id}' å¤±æ•—: {mem_clear_err}")

        history_file = Path(f"chat_history/{session_id}.json")
        if history_file.exists():
            try:
                history_file.unlink()
                deleted_from_file = True
                logger.info(f"ğŸ—‘ï¸ å·²åˆªé™¤å°è©±æ­·å²æª”æ¡ˆ: {history_file}")
            except OSError as file_del_err:
                logger.error(f"âŒ åˆªé™¤å°è©±æ­·å²æª”æ¡ˆ '{history_file}' å¤±æ•—: {file_del_err}")

        if deleted_from_memory or deleted_from_file:
            return {"message": f"âœ… å·²æ¸…é™¤ {session_id} çš„å°è©±è¨˜æ†¶" + (" (éƒ¨åˆ†å¤±æ•—ï¼Œè«‹æŸ¥çœ‹æ—¥èªŒ)" if not (deleted_from_memory and deleted_from_file) else "")}
        else:
            logger.warning(f"âš ï¸ å˜—è©¦æ¸…é™¤ session '{session_id}'ï¼Œä½†è¨˜æ†¶é«”èˆ‡æª”æ¡ˆçš†ä¸å­˜åœ¨æˆ–åˆªé™¤å¤±æ•—")
            raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°æˆ–ç„¡æ³•æ¸…é™¤æŒ‡å®šçš„å°è©±è¨˜éŒ„")

    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.error(f"âŒ æ¸…é™¤å°è©±è¨˜æ†¶æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤ ({session_id}): {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æ¸…é™¤è¨˜æ†¶å¤±æ•—: {str(e)}")


@app.get("/api/stats")
async def get_stats():
    """å–å¾—ç³»çµ±çµ±è¨ˆè³‡è¨Š"""
    try:
        vector_count = -1
        try:
            collection_info = qdrant_client.get_collection(COLLECTION_NAME)
            vector_count = collection_info.vectors_count
        except Exception as q_err:
             logger.error(f"âŒ ç„¡æ³•å¾ Qdrant å–å¾—é›†åˆè³‡è¨Š: {q_err}")

        return {
            "collection_name": COLLECTION_NAME,
            "total_vectors": vector_count,
            "active_memory_sessions": len(memory_store),
            "total_history_files": len(list(Path("chat_history").glob("*.json"))),
            "framework": "LangChain",
            "llm_model": llm.model,
            "embedding_model": embeddings.model_name,
            "vector_db": "Qdrant",
            "components": {
                "agents": "âœ… ReAct Agent" if agent else "âŒ Agent Failed",
                "memory": "âœ… ConversationBufferMemory + FileChatMessageHistory",
                "rag": "âœ… ConversationalRetrievalChain",
                "tools": len(tools)
            }
        }
    except Exception as e:
        logger.error(f"âŒ å–å¾—ç³»çµ±çµ±è¨ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç„¡æ³•å–å¾—ç³»çµ±çµ±è¨ˆ: {str(e)}")

# --- å•Ÿå‹•èˆ‡é—œé–‰äº‹ä»¶ä¿æŒä¸è®Š ---
@app.on_event("startup")
async def startup_event():
    # ç¢ºä¿æ‰€æœ‰éœ€è¦çš„ç›®éŒ„éƒ½å­˜åœ¨
    Path("chat_history").mkdir(exist_ok=True)
    Path("generated_content").mkdir(exist_ok=True)
    Path("logs").mkdir(exist_ok=True)
    Path("documents").mkdir(exist_ok=True)

    logger.info("="*50)
    logger.info("ğŸš€ PAIS ç³»çµ±å•Ÿå‹• (LangChain Powered)")
    logger.info(f" FastAPI ç‰ˆæœ¬: {app.version}")
    logger.info(f"ğŸ“ Qdrant Host: {QDRANT_HOST}:{QDRANT_PORT}")
    logger.info(f"ğŸ“š Qdrant é›†åˆ: {COLLECTION_NAME}")
    logger.info(f"ğŸ§  LLM æ¨¡å‹: {llm.model}")
    logger.info(f"ğŸ”¡ Embedding æ¨¡å‹: {embeddings.model_name} (ç¶­åº¦: 768)")
    logger.info(f"ğŸ¤– Agent ç‹€æ…‹: {'âœ… å·²å•Ÿç”¨' if agent else 'âŒ å•Ÿå‹•å¤±æ•—'}")
    logger.info(f"ğŸ› ï¸ Agent å·¥å…·æ•¸é‡: {len(tools)}")
    logger.info(f"ğŸ’¾ Memory é¡å‹: ConversationBufferMemory + FileChatMessageHistory")
    logger.info("="*50)

@app.on_event("shutdown")
async def shutdown_event():
    logger.info("="*50)
    logger.info("â¹ PAIS ç³»çµ±æ­£åœ¨é—œé–‰...")
    # å¯ä»¥åœ¨é€™è£¡åŠ å…¥è³‡æºæ¸…ç†çš„ç¨‹å¼ç¢¼ï¼Œä¾‹å¦‚é—œé–‰è³‡æ–™åº«é€£æ¥
    logger.info("="*50)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=False)