"""
æ–‡æ¡ˆç”Ÿæˆæœå‹™ (å¸¶è¨˜æ†¶åŠŸèƒ½)
éµå¾ª Single Responsibility Principleï¼šå°ˆæ³¨æ–¼æ–‡æ¡ˆç”Ÿæˆé‚è¼¯
"""
import os
from typing import Optional
from langchain_google_genai import GoogleGenerativeAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import Qdrant
from qdrant_client import QdrantClient
from langchain_community.embeddings import HuggingFaceEmbeddings
from loguru import logger

from .memory_manager import StaffMemoryManager


class ContentGenerator:
    """æ–‡æ¡ˆç”Ÿæˆå™¨ (å¸¶è¨˜æ†¶å­¸ç¿’åŠŸèƒ½)"""
    
    def __init__(
        self, 
        memory_manager: StaffMemoryManager,
        gemini_api_key: Optional[str] = None,
        qdrant_host: str = "qdrant",
        qdrant_port: int = 6333
    ):
        self.memory_manager = memory_manager
        
        # åˆå§‹åŒ– LLM
        api_key = gemini_api_key or os.getenv("GEMINI_API_KEY")
        self.llm = GoogleGenerativeAI(
            model="gemini-2.0-flash",
            google_api_key=api_key,
            temperature=0.7,
            max_output_tokens=1024
        )
        
        # åˆå§‹åŒ–å‘é‡è³‡æ–™åº« (å…±ç”¨çŸ¥è­˜åº«)
        embeddings = HuggingFaceEmbeddings(
            model_name="moka-ai/m3e-base",
            model_kwargs={'device': 'cpu'}
        )
        
        qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port)
        self.vectorstore = Qdrant(
            client=qdrant_client,
            collection_name="pais_knowledge_base",
            embeddings=embeddings
        )
        
        # å»ºç«‹ Prompt æ¨¡æ¿
        self.prompt = self._build_prompt()
        logger.info("âœ… æ–‡æ¡ˆç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _build_prompt(self) -> PromptTemplate:
        """å»ºç«‹æ–‡æ¡ˆç”Ÿæˆ Prompt"""
        return PromptTemplate(
            template="""ä½ æ˜¯æ¡ƒåœ’å¸‚é•·çš„å°ˆå±¬æ–‡æ¡ˆåŠ©ç†ã€‚

ã€éå¾€å­¸ç¿’è¨˜éŒ„ã€‘
{chat_history}

ã€æœ¬æ¬¡ä»»å‹™ã€‘
- ä¸»é¡Œï¼š{topic}
- é¢¨æ ¼ï¼š{style}
- é•·åº¦ï¼š{length}

ã€åƒè€ƒè³‡æ–™ã€‘
{context}

ã€è¦æ±‚ã€‘
1. å®Œå…¨æ¨¡ä»¿å¸‚é•·çš„èªæ°£å’Œç”¨å­—é£è©
2. æ ¹æ“šéå¾€è¨˜éŒ„æŒçºŒå­¸ç¿’å’Œå„ªåŒ–
3. é¢¨æ ¼èªªæ˜ï¼š
   - formal (æ­£å¼)ï¼šå®˜æ–¹æ­£å¼ç”¨èªï¼Œé©åˆæ”¿ç­–å®£å¸ƒ
   - casual (è¼•é¬†)ï¼šè¦ªåˆ‡å£å»ï¼Œé©åˆæ—¥å¸¸äº’å‹•
   - humorous (å¹½é»˜)ï¼šåŠ å…¥è¼•é¬†å¹½é»˜å…ƒç´ 
4. é•·åº¦è¦æ±‚ï¼š
   - shortï¼š50-100å­—
   - mediumï¼š150-300å­—
   - longï¼š400-600å­—
5. å…§å®¹å¿…é ˆåŸºæ–¼åƒè€ƒè³‡æ–™ï¼Œé¿å…è™›æ§‹

è«‹ç›´æ¥ç”Ÿæˆæ–‡æ¡ˆï¼Œä¸è¦æœ‰å…¶ä»–èªªæ˜ï¼š""",
            input_variables=["topic", "style", "length", "context", "chat_history"]
        )
    
    async def generate(
        self, 
        task_id: str, 
        topic: str, 
        style: str, 
        length: str
    ) -> str:
        """
        ç”Ÿæˆæ–‡æ¡ˆ
        
        Args:
            task_id: ä»»å‹™ ID
            topic: æ–‡æ¡ˆä¸»é¡Œ
            style: é¢¨æ ¼ (formal/casual/humorous)
            length: é•·åº¦ (short/medium/long)
        
        Returns:
            ç”Ÿæˆçš„æ–‡æ¡ˆå…§å®¹
        """
        try:
            # å¾çŸ¥è­˜åº«æª¢ç´¢ç›¸é—œè³‡æ–™
            context = self._retrieve_context(topic)
            
            # å–å¾—è¨˜æ†¶
            memory = self.memory_manager.get_memory(task_id)
            
            # å»ºç«‹ Chain
            chain = LLMChain(
                llm=self.llm,
                prompt=self.prompt,
                memory=memory,
                verbose=True
            )
            
            # ç”Ÿæˆæ–‡æ¡ˆ
            logger.info(f"ğŸš€ é–‹å§‹ç”Ÿæˆæ–‡æ¡ˆ: {task_id} - {topic}")
            result = await chain.ainvoke({
                "topic": topic,
                "style": style,
                "length": length,
                "context": context
            })
            
            content = result["text"].strip()
            
            # è¨˜éŒ„ç”Ÿæˆçµæœåˆ°è¨˜æ†¶
            self.memory_manager.add_generation_record(
                task_id, topic, style, content
            )
            
            logger.info(f"âœ… æ–‡æ¡ˆç”Ÿæˆå®Œæˆ: {task_id} ({len(content)} å­—)")
            return content
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æ¡ˆç”Ÿæˆå¤±æ•—: {task_id} - {e}")
            raise
    
    def _retrieve_context(self, topic: str, k: int = 3) -> str:
        """å¾çŸ¥è­˜åº«æª¢ç´¢ç›¸é—œè³‡æ–™"""
        try:
            retriever = self.vectorstore.as_retriever(search_kwargs={"k": k})
            docs = retriever.invoke(topic)
            
            if docs:
                context = "\n\n".join([doc.page_content for doc in docs])
                logger.info(f"ğŸ“š æª¢ç´¢åˆ° {len(docs)} ç­†ç›¸é—œè³‡æ–™")
                return context[:2000]  # é™åˆ¶é•·åº¦
            else:
                logger.warning("âš ï¸ çŸ¥è­˜åº«ä¸­æœªæ‰¾åˆ°ç›¸é—œè³‡æ–™")
                return "ï¼ˆç„¡ç‰¹å®šåƒè€ƒè³‡æ–™ï¼‰"
                
        except Exception as e:
            logger.error(f"âŒ æª¢ç´¢çŸ¥è­˜åº«å¤±æ•—: {e}")
            return "ï¼ˆæª¢ç´¢å¤±æ•—ï¼‰"
    
    def save_edit_feedback(self, task_id: str, original: str, edited: str):
        """å„²å­˜äººå·¥ä¿®æ”¹ä½œç‚ºå­¸ç¿’æ¨£æœ¬"""
        self.memory_manager.save_feedback(task_id, original, edited)
        logger.info(f"ğŸ“š å·²å„²å­˜ä¿®æ”¹è¨˜éŒ„ä½œç‚ºå­¸ç¿’æ¨£æœ¬: {task_id}")