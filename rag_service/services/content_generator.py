import os
from typing import Optional
from langchain_google_genai import GoogleGenerativeAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import Qdrant
from qdrant_client import QdrantClient
from langchain_community.embeddings import HuggingFaceEmbeddings
from loguru import logger

from .memory_manager import StaffMemoryManager


class ContentGenerator:
    """æ–‡æ¡ˆç”Ÿæˆå™¨ (å¸¶è¨˜æ†¶å­¸ç¿’åŠŸèƒ½)"""
    
    def __init__(
        self, 
        memory_manager: StaffMemoryManager,
        gemini_api_key: Optional[str] = None,
        qdrant_host: str = "qdrant",
        qdrant_port: int = 6333
    ):
        self.memory_manager = memory_manager
        
        # åˆå§‹åŒ– LLM
        api_key = gemini_api_key or os.getenv("GEMINI_API_KEY")
        self.llm = GoogleGenerativeAI(
            model="gemini-2.0-flash",
            google_api_key=api_key,
            temperature=0.7,
            max_output_tokens=1024
        )
        
        # åˆå§‹åŒ–å‘é‡è³‡æ–™åº« (å…±ç”¨çŸ¥è­˜åº«)
        embeddings = HuggingFaceEmbeddings(
            model_name="moka-ai/m3e-base",
            model_kwargs={'device': 'cpu'}
        )
        
        qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port)
        self.vectorstore = Qdrant(
            client=qdrant_client,
            collection_name="pais_knowledge_base",
            embeddings=embeddings
        )
        
        # å»ºç«‹ Prompt æ¨¡æ¿
        self.prompt = self._build_prompt()
        logger.info("âœ…æ–‡æ¡ˆç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _build_prompt(self) -> PromptTemplate:
        """å»ºç«‹æ–‡æ¡ˆç”Ÿæˆ Prompt"""
        return PromptTemplate(
            template="""ä½ æ˜¯å¸‚é•·å¼µå–„æ”¿çš„æ•¸ä½åˆ†èº«èˆ‡é¦–å¸­æ–‡è†½ã€‚

ã€æ ¸å¿ƒæ€ç¶­ï¼šå·¥ç¨‹å¸«äººæ–‡ä¸»ç¾©ã€‘
- è¦–è§’ï¼šç†å·¥è…¦ï¼ˆæ•¸æ“šå¯¦è­‰ï¼‰+ çˆ¶æ¯å¿ƒï¼ˆåŒç†é—œæ‡·ï¼‰
- è²ç´‹ï¼šæ²ˆç©©è¦ªåˆ‡ã€å‹™å¯¦ä¸ç©ºæ³›
- ä¿¡å¿µï¼šèªªåˆ°åšåˆ°ã€åœ˜éšŠå…±åŒåŠªåŠ›
- èªåº«ï¼šã€Œå„ä½é„‰è¦ªã€ã€Œå¸‚åºœåœ˜éšŠå…¨åŠ›ä»¥èµ´ã€ã€Œè®“æˆ‘å€‘ä¸€èµ·æ‰“æ‹¼ã€ã€Œè¬è¬å¤§å®¶çš„æ”¯æŒã€

ã€å­¸ç¿’è¨˜æ†¶ã€‘
{chat_history}
**å¾éå¾€ä¿®æ”¹ä¸­å­¸ç¿’å¹•åƒšåå¥½ï¼Œé¿å…é‡è¤‡éŒ¯èª¤ï¼Œä¿æŒé¢¨æ ¼ä¸€è‡´ã€‚**

ã€ä»»å‹™ã€‘ä¸»é¡Œï¼š{topic} | é¡å‹ï¼š{style} | é•·åº¦ï¼š{length}

ã€çŸ¥è­˜åº«ã€‘
{context}
**æ•¸æ“šã€æ—¥æœŸã€æ”¿ç­–åç¨±å¿…é ˆåŸºæ–¼ä¸Šè¿°è³‡æ–™ï¼Œä¸å¾—ç·¨é€ ã€‚**

ã€æ–‡æ¡ˆé¡å‹å”è­°ã€‘

**pressï¼ˆæ–°èç¨¿ï¼‰**
- ç¬¬ä¸‰äººç¨±å®¢è§€å ±å°ï¼ˆã€Œå¸‚é•·å¼µå–„æ”¿è¡¨ç¤ºã€ã€Œæ¡ƒåœ’å¸‚æ”¿åºœå®£å¸ƒã€ï¼‰
- çµæ§‹ï¼š5W1H å°è¨€ â†’ å¼•è¿°ç™¼è¨€ â†’ æ”¿ç­–ç´°ç¯€ â†’ é æœŸæ•ˆç›Š
- ç¦ç”¨ç¬¬ä¸€äººç¨±ã€Œæˆ‘ã€ã€Œæˆ‘å€‘ã€
- ç¯„ä¾‹ï¼šã€Œæ¡ƒåœ’å¸‚é•·å¼µå–„æ”¿ä»Šæ—¥å®£å¸ƒ...ã€

**speechï¼ˆæ¼”è¬›ç¨¿ï¼‰**
- ç¬¬ä¸€äººç¨±æƒ…æ„Ÿé€£çµ
- çµæ§‹ï¼šé–‹å ´å•å€™ â†’ ä¸»é¡Œé‹ªé™³ â†’ æ ¸å¿ƒè«–è¿° â†’ æƒ…æ„Ÿè™Ÿå¬ â†’ æ„Ÿè¬çµèª
- å£èªæŠ€å·§ï¼šçŸ­å¥ã€é‡è¤‡å¼·èª¿ã€è¨­å•ä¿®è¾­
- ç¯„ä¾‹ï¼šã€Œå„ä½é„‰è¦ªã€å¸‚æ°‘æœ‹å‹ï¼Œå¤§å®¶å¥½ï¼ã€

**facebookï¼ˆFacebook è²¼æ–‡ï¼‰**
- ç¬¬ä¸€äººç¨±è¦ªæ°‘äº’å‹•ï¼Œå¯ç”¨è¡¨æƒ…ç¬¦è™Ÿã€å•å¥
- çµæ§‹ï¼šå¸ç›é–‹å ´ â†’ ç”Ÿæ´»åŒ–æ•˜äº‹ â†’ æ”¿ç­–èªªæ˜ â†’ äº’å‹•è™Ÿå¬
- ç¯„ä¾‹ï¼šã€Œå¤§å®¶å¥½ï¼ä»Šå¤©è¦è·Ÿå„ä½åˆ†äº«ä¸€å€‹å¥½æ¶ˆæ¯ğŸ˜Šã€

**instagramï¼ˆInstagram è²¼æ–‡ï¼‰**
- ç¬¬ä¸€äººç¨±è¦–è¦ºç‚ºä¸»ï¼Œç²¾ç°¡æ˜å¿«
- çµæ§‹ï¼šç°¡çŸ­é–‹å ´ â†’ æ ¸å¿ƒè¨Šæ¯ï¼ˆ2-3å¥ï¼‰â†’ hashtag
- ç¯„ä¾‹ï¼šã€Œæ¡ƒåœ’çš„æ”¹è®Šï¼Œä½ çœ‹è¦‹äº†å—ï¼Ÿâœ¨#æ¡ƒåœ’ #å¸‚æ”¿å»ºè¨­ã€

**posterï¼ˆå®£å‚³æµ·å ±ï¼‰**
- ç„¡äººç¨±æˆ–ç¥ˆä½¿å¥ï¼Œ20-50 å­—æœ€ä½³
- çµæ§‹ï¼šä¸»æ¨™ï¼ˆæ ¸å¿ƒè¨´æ±‚ï¼‰+ å‰¯æ¨™ï¼ˆè£œå……èªªæ˜ï¼‰
- ç¯„ä¾‹ï¼šã€Œèªªåˆ°åšåˆ°ï¼Œç‚ºæ¡ƒåœ’æ‰“æ‹¼ | å¸‚é•·å¼µå–„æ”¿ï¼Œèˆ‡æ‚¨ä¸€èµ·å»ºè¨­å¹¸ç¦æ¡ƒåœ’ã€

ã€é•·åº¦è¦ç¯„ã€‘
- short (50-100å­—)ï¼šInstagramã€æµ·å ±
- medium (150-300å­—)ï¼šFacebookã€æ–°èç¨¿å°è¨€
- long (400-600å­—)ï¼šå®Œæ•´æ–°èç¨¿ã€æ¼”è¬›ç¨¿

ã€å®‰å…¨é–¥ã€‘
- æ•¸æ“šé›¶å®¹å¿ï¼šä¸ç·¨é€ æ•¸å­—ã€æ—¥æœŸ
- æ‰¿è«¾é‚Šç•Œï¼šæ…ç”¨ã€Œä¸€å®šã€ã€Œä¿è­‰ã€ï¼Œç”¨ã€ŒæœƒåŠªåŠ›ã€ã€ŒæŒçºŒæ¨å‹•ã€
- å½¢è±¡é˜²è­·ï¼šä¸æ¶‰åŠé¸èˆ‰ã€æ”¿é»¨æ”»æ“Š
- å“è³ªé–¥ï¼šé‚è¼¯é€šé †ã€é¢¨æ ¼ä¸€è‡´ã€ç¬¦åˆé•·åº¦è¦æ±‚

**åªè¼¸å‡ºæ–‡æ¡ˆæœ¬èº«ï¼Œä¸è¦æœ‰ä»»ä½•èªªæ˜æˆ–å‚™è¨»ã€‚**
é–‹å§‹ç”Ÿæˆï¼š""",
            input_variables=["topic", "style", "length", "context", "chat_history"]
        )
    
    async def generate(
        self, 
        task_id: str, 
        topic: str, 
        style: str, 
        length: str
    ) -> str:
        """
        ç”Ÿæˆæ–‡æ¡ˆ
        
        Args:
            task_id: ä»»å‹™ ID
            topic: æ–‡æ¡ˆä¸»é¡Œ
            style: é¢¨æ ¼ (formal/casual/humorous)
            length: é•·åº¦ (short/medium/long)
        
        Returns:
            ç”Ÿæˆçš„æ–‡æ¡ˆå…§å®¹
        """
        try:
            # å¾çŸ¥è­˜åº«æª¢ç´¢ç›¸é—œè³‡æ–™
            context = self._retrieve_context(topic)

            # å–å¾—è¨˜æ†¶ä¸¦æ‰‹å‹•æå– chat_history
            memory = self.memory_manager.get_memory(task_id)
            chat_history = memory.load_memory_variables({}).get("chat_history", "")

            # å»ºç«‹ Chainï¼ˆä¸ä½¿ç”¨è‡ªå‹• memoryï¼Œæ‰‹å‹•å‚³å…¥ chat_historyï¼‰
            chain = LLMChain(
                llm=self.llm,
                prompt=self.prompt,
                verbose=True
            )

            # ç”Ÿæˆæ–‡æ¡ˆ
            logger.info(f"é–‹å§‹ç”Ÿæˆæ–‡æ¡ˆ: {task_id} - {topic}")
            result = await chain.ainvoke({
                "topic": topic,
                "style": style,
                "length": length,
                "context": context,
                "chat_history": chat_history
            })

            content = result["text"].strip()

            # æ‰‹å‹•ä¿å­˜åˆ°è¨˜æ†¶
            memory.save_context(
                {"input": f"ç”Ÿæˆæ–‡æ¡ˆ - ä¸»é¡Œ: {topic}, é¢¨æ ¼: {style}, é•·åº¦: {length}"},
                {"text": content}
            )

            # è¨˜éŒ„ç”Ÿæˆçµæœåˆ°è¨˜æ†¶
            self.memory_manager.add_generation_record(
                task_id, topic, style, content
            )

            logger.info(f"âœ…æ–‡æ¡ˆç”Ÿæˆå®Œæˆ: {task_id} ({len(content)} å­—)")
            return content
            
        except Exception as e:
            logger.error(f"âŒæ–‡æ¡ˆç”Ÿæˆå¤±æ•—: {task_id} - {e}")
            raise
    
    def _retrieve_context(self, topic: str, k: int = 3) -> str:
        """å¾çŸ¥è­˜åº«æª¢ç´¢ç›¸é—œè³‡æ–™"""
        try:
            retriever = self.vectorstore.as_retriever(search_kwargs={"k": k})
            docs = retriever.invoke(topic)
            
            if docs:
                context = "\n\n".join([doc.page_content for doc in docs])
                logger.info(f"æª¢ç´¢åˆ° {len(docs)} ç­†ç›¸é—œè³‡æ–™")
                return context[:2000]
            else:
                logger.warning("âš ï¸çŸ¥è­˜åº«ä¸­æœªæ‰¾åˆ°ç›¸é—œè³‡æ–™")
                return "ï¼ˆç„¡ç‰¹å®šåƒè€ƒè³‡æ–™ï¼‰"
                
        except Exception as e:
            logger.error(f"âŒæª¢ç´¢çŸ¥è­˜åº«å¤±æ•—: {e}")
            return "ï¼ˆæª¢ç´¢å¤±æ•—ï¼‰"
    
    def save_edit_feedback(self, task_id: str, original: str, edited: str):
        """å„²å­˜äººå·¥ä¿®æ”¹ä½œç‚ºå­¸ç¿’æ¨£æœ¬"""
        self.memory_manager.save_feedback(task_id, original, edited)
        logger.info(f"å·²å„²å­˜ä¿®æ”¹è¨˜éŒ„ä½œç‚ºå­¸ç¿’æ¨£æœ¬: {task_id}")