import os
import json
import re # åŒ¯å…¥æ­£è¦è¡¨é”å¼æ¨¡çµ„
from datetime import datetime
from typing import List, Optional, Dict, Any
from pathlib import Path

from fastapi import FastAPI, HTTPException, UploadFile, File, Depends, Header, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ==================== LangChain æ ¸å¿ƒ ====================
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    PyPDFLoader, Docx2txtLoader, TextLoader
)
from langchain_community.vectorstores import Qdrant
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams

# ==================== LangChain Agents ====================
from langchain.agents import AgentExecutor, create_react_agent, Tool
from langchain.prompts import PromptTemplate

# ==================== LangChain Memory ====================
from langchain.memory import ConversationBufferMemory
from langchain_community.chat_message_histories import FileChatMessageHistory

# ==================== LangChain Chains ====================
from langchain.chains import (
    ConversationalRetrievalChain,
    LLMChain
)

from dotenv import load_dotenv
from loguru import logger

# è¼‰å…¥æ•¸æ“šåº«è¼”åŠ©é¡
from utils.db_helper import StaffDatabase

# è¼‰å…¥ ChatService å’Œ Prompts
from services.chat_service import ChatService
from prompts import PUBLIC_AGENT_PROMPT, STAFF_AGENT_PROMPT

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# ==================== é…ç½® ====================
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", 6333))
ADMIN_PASSWORD = os.getenv("ADMIN_PASSWORD", "admin123456")
COLLECTION_NAME = "pais_knowledge_base"

# è¨­å®šæ—¥èªŒ
logger.add("logs/pais_{time}.log", rotation="1 day", retention="30 days")

# ==================== FastAPI æ‡‰ç”¨ ====================
app = FastAPI(
    title="PAIS - æ”¿å‹™åˆ†èº«æ™ºèƒ½ç³»çµ± (LangChain Powered)",
    description="åŸºæ–¼ LangChain çš„å¸‚é•·èŠå¤©æ©Ÿå™¨äºº (Agents + Memory + RAG + Tools)",
    version="2.0.6" # ç‰ˆæœ¬æ›´æ–°
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== LangChain åˆå§‹åŒ– ====================

# Gemini LLM
llm = GoogleGenerativeAI(
    model="gemini-2.0-flash",
    google_api_key=GEMINI_API_KEY,
    temperature=0.7,
    max_output_tokens=2048
)

# Embeddings (ä½¿ç”¨ moka-ai/m3e-base)
embeddings = HuggingFaceEmbeddings(
    model_name="moka-ai/m3e-base",
    model_kwargs={'device': 'cpu'}
)

# Qdrant å‘é‡è³‡æ–™åº«
qdrant_client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)

try:
    qdrant_client.get_collection(COLLECTION_NAME)
    logger.info(f"âœ… é›†åˆ '{COLLECTION_NAME}' å·²å­˜åœ¨")
except Exception:
    logger.warning(f"âš ï¸ é›†åˆ '{COLLECTION_NAME}' ä¸å­˜åœ¨ï¼Œå˜—è©¦å»ºç«‹...")
    try:
        qdrant_client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=768, distance=Distance.COSINE) # ç¶­åº¦ç‚º 768
        )
        logger.info(f"âœ… å·²å»ºç«‹æ–°é›†åˆ '{COLLECTION_NAME}'")
    except Exception as create_err:
        logger.error(f"âŒ å»ºç«‹é›†åˆ '{COLLECTION_NAME}' å¤±æ•—: {create_err}")

vectorstore = Qdrant(
    client=qdrant_client,
    collection_name=COLLECTION_NAME,
    embeddings=embeddings
)

# ==================== è¨ªå®¢è¨ˆæ•¸å™¨æ•¸æ“šåº« ====================
db = StaffDatabase()

# ==================== LangChain Memory ç®¡ç† ====================
memory_store: Dict[str, ConversationBufferMemory] = {}

def get_memory(session_id: str) -> ConversationBufferMemory:
    """å–å¾—æˆ–å»ºç«‹å°è©±è¨˜æ†¶"""
    if session_id not in memory_store:
        try:
            Path("chat_history").mkdir(exist_ok=True)
            history_file = f"chat_history/{session_id}.json"
            message_history = FileChatMessageHistory(history_file)

            memory_store[session_id] = ConversationBufferMemory(
                chat_memory=message_history,
                memory_key="chat_history",
                return_messages=True
            )
            logger.info(f"ğŸ§  å»ºç«‹æ–°è¨˜æ†¶: {session_id} (æª”æ¡ˆ: {history_file})")
        except Exception as mem_err:
            logger.error(f"âŒ å»ºç«‹è¨˜æ†¶é«”å¤±æ•— ({session_id}): {mem_err}")
            return ConversationBufferMemory(memory_key="chat_history", return_messages=True)
            
    return memory_store[session_id]

# ==================== LangChain Tools ====================

def search_knowledge_base(query: str) -> str:
    """æœå°‹çŸ¥è­˜åº«å·¥å…·"""
    logger.info(f"ğŸ› ï¸ ä½¿ç”¨å·¥å…· [æœå°‹çŸ¥è­˜åº«]ï¼ŒæŸ¥è©¢: {query}")
    try:
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        docs = retriever.invoke(query) # ä½¿ç”¨ invoke
        if docs:
            # åªå– page_contentï¼Œé¿å… metadata éé•·
            result = "\n\n".join([doc.page_content for doc in docs])
            logger.info(f"âœ… å·¥å…· [æœå°‹çŸ¥è­˜åº«] æ‰¾åˆ° {len(docs)} ç­†è³‡æ–™")
            # é™åˆ¶å›å‚³çµ¦ Agent çš„é•·åº¦ï¼Œé¿å… Prompt éé•·
            max_obs_length = 1500 
            if len(result) > max_obs_length:
                 result = result[:max_obs_length] + "... (å…§å®¹éé•·æˆªæ–·)"
            return f"æ‰¾åˆ°ç›¸é—œè³‡æ–™ï¼š\n{result}"
        else:
            logger.warning(f"âš ï¸ å·¥å…· [æœå°‹çŸ¥è­˜åº«] æœªæ‰¾åˆ°è³‡æ–™ for query: {query}")
            return "çŸ¥è­˜åº«ä¸­æ‰¾ä¸åˆ°èˆ‡æ­¤ç›´æ¥ç›¸é—œçš„è³‡æ–™ã€‚"
    except Exception as e:
        logger.error(f"âŒ å·¥å…· [æœå°‹çŸ¥è­˜åº«] åŸ·è¡ŒéŒ¯èª¤: {e}", exc_info=True)
        return f"æœå°‹çŸ¥è­˜åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"

def get_policy_info(policy_name: str) -> str:
    """å–å¾—ç‰¹å®šæ”¿ç­–è³‡è¨Šå·¥å…·"""
    logger.info(f"ğŸ› ï¸ ä½¿ç”¨å·¥å…· [æŸ¥è©¢æ”¿ç­–]ï¼Œæ”¿ç­–åç¨±: {policy_name}")
    try:
        docs = vectorstore.similarity_search(policy_name, k=1) # åªå–æœ€ç›¸é—œçš„ 1 ç­†
        if docs:
            result = docs[0].page_content
            logger.info(f"âœ… å·¥å…· [æŸ¥è©¢æ”¿ç­–] æ‰¾åˆ°è³‡æ–™ for policy: {policy_name}")
            # é™åˆ¶å›å‚³çµ¦ Agent çš„é•·åº¦
            max_obs_length = 1500
            if len(result) > max_obs_length:
                 result = result[:max_obs_length] + "... (å…§å®¹éé•·æˆªæ–·)"
            return f"é—œæ–¼ '{policy_name}' çš„è³‡è¨Šï¼š{result}"
        else:
            logger.warning(f"âš ï¸ å·¥å…· [æŸ¥è©¢æ”¿ç­–] æœªæ‰¾åˆ°è³‡æ–™ for policy: {policy_name}")
            return f"çŸ¥è­˜åº«ä¸­æ‰¾ä¸åˆ°åç‚º '{policy_name}' çš„ç‰¹å®šæ”¿ç­–è³‡è¨Šã€‚"
    except Exception as e:
        logger.error(f"âŒ å·¥å…· [æŸ¥è©¢æ”¿ç­–] åŸ·è¡ŒéŒ¯èª¤: {e}", exc_info=True)
        return f"æŸ¥è©¢æ”¿ç­– '{policy_name}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"

# å®šç¾© Agent å·¥å…·
tools = [
    Tool(
        name="æœå°‹çŸ¥è­˜åº«",
        func=search_knowledge_base,
        description="ç•¶ä½ éœ€è¦å›ç­”é—œæ–¼å¸‚é•·çš„**æ”¿ç­–ã€ç†å¿µã€æ–½æ”¿å ±å‘Šã€å…¬é–‹ç™¼è¨€ã€å€‹äººèƒŒæ™¯**æˆ–**æ¡ƒåœ’å¸‚æ”¿ç›¸é—œå•é¡Œ**æ™‚ä½¿ç”¨ã€‚**è¼¸å…¥ï¼š** å…·é«”çš„å•é¡Œæˆ–æ¸…æ™°çš„é—œéµå­—è©çµ„ (ä¾‹å¦‚ï¼š'æ¡ƒåœ’å¸‚çš„äº¤é€šæ”¿ç­–æœ‰å“ªäº›ï¼Ÿ', 'å¸‚é•·å°æ–¼é’å¹´å°±æ¥­çš„çœ‹æ³•', 'èªªæ˜ç¤¾æœƒä½å®…çš„é€²åº¦')ã€‚**ä¸è¦**åªè¼¸å…¥æ¨¡ç³Šçš„å–®è©ã€‚"
    ),
    Tool(
        name="æŸ¥è©¢ç‰¹å®šæ”¿ç­–åç¨±",
        func=get_policy_info,
        description="ç•¶ä½¿ç”¨è€…**æ˜ç¢º**æåˆ°ä¸€å€‹**å…·é«”çš„æ”¿ç­–åç¨±**ï¼Œè€Œä½ éœ€è¦æŸ¥æ‰¾è©²æ”¿ç­–çš„è©³ç´°å…§å®¹æ™‚ä½¿ç”¨ã€‚**è¼¸å…¥ï¼š** å®Œæ•´çš„æ”¿ç­–åç¨± (ä¾‹å¦‚ï¼š'äº”æ­²å¹¼å…’æ•™è‚²åŠ©å­¸é‡‘', 'åœ‹ä¸­å°å…è²»ç‡Ÿé¤Šåˆé¤')ã€‚å¦‚æœåªæ˜¯å•æŸå€‹é ˜åŸŸçš„æ”¿ç­– (å¦‚ 'äº¤é€šæ”¿ç­–')ï¼Œè«‹ä½¿ç”¨ã€Œæœå°‹çŸ¥è­˜åº«ã€ã€‚"
    ),
]

# ==================== LangChain Agent å®šç¾© ====================

# å‰µå»º Agent Prompt Templateï¼ˆä½¿ç”¨å¾ prompts æ¨¡çµ„å°å…¥çš„ Promptï¼‰
agent_prompt = PromptTemplate(
    template=PUBLIC_AGENT_PROMPT,
    input_variables=["input", "chat_history", "agent_scratchpad", "tools", "tool_names"]
)

staff_agent_prompt = PromptTemplate(
    template=STAFF_AGENT_PROMPT,
    input_variables=["input", "chat_history", "agent_scratchpad", "tools", "tool_names"]
)

# å‰µå»º Agentï¼ˆå…¬çœ¾ç‰ˆ - å–„å¯¶ï¼‰
try:
    agent = create_react_agent(
        llm=llm,
        tools=tools,
        prompt=agent_prompt
    )
    logger.info("âœ… ReAct Agent (å…¬çœ¾ç‰ˆ) å‰µå»ºæˆåŠŸ")
except Exception as agent_create_err:
    try:
        logger.error(f"âŒ å‰µå»º Agent å¤±æ•—: {str(agent_create_err)}", exc_info=True)
    except Exception as log_err:
        logger.error(f"âŒ å‰µå»º Agent å¤±æ•—ï¼Œä¸” Logger ä¹Ÿç™¼ç”ŸéŒ¯èª¤: {log_err}")
    agent = None

# å‰µå»º Staff Agentï¼ˆå¹•åƒšç‰ˆï¼‰
try:
    staff_agent = create_react_agent(
        llm=llm,
        tools=tools,
        prompt=staff_agent_prompt
    )
    logger.info("âœ… ReAct Agent (å¹•åƒšç‰ˆ) å‰µå»ºæˆåŠŸ")
except Exception as staff_agent_create_err:
    try:
        logger.error(f"âŒ å‰µå»º Staff Agent å¤±æ•—: {str(staff_agent_create_err)}", exc_info=True)
    except Exception as log_err:
        logger.error(f"âŒ å‰µå»º Staff Agent å¤±æ•—ï¼Œä¸” Logger ä¹Ÿç™¼ç”ŸéŒ¯èª¤: {log_err}")
    staff_agent = None

# ==================== ChatService åˆå§‹åŒ– ====================

# å‰µå»º ChatService å¯¦ä¾‹
chat_service = ChatService(
    llm=llm,
    vectorstore=vectorstore,
    tools=tools,
    agent=agent,
    staff_agent=staff_agent,
    rag_prompt=None  # RAG prompt å°‡åœ¨å¾Œé¢å®šç¾©å¾Œæ›´æ–°
)

# ==================== Pydantic æ¨¡å‹ ====================
# (ä¿æŒä¸è®Š)
class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = "default"
    use_agent: bool = True
    role: str = "public"  # "public" æˆ– "staff"ï¼Œæ±ºå®š AI çš„èº«ä»½

class ChatResponse(BaseModel):
    reply: str
    sources: Optional[List[str]] = []
    session_id: str
    timestamp: str
    thought_process: Optional[str] = None # æ”¹ç‚ºå­—ä¸²ä»¥å®¹ç´éŒ¯èª¤è¨Šæ¯æˆ–æ­¥é©Ÿ

class ContentGenerationRequest(BaseModel):
    topic: str
    style: str = "æ­£å¼"
    length: str = "ä¸­"
    context: Optional[str] = None

class IngestRequest(BaseModel):
    folder_path: str = "documents"

class VisitorStatsResponse(BaseModel):
    month: str
    count: int
    last_reset: str

# ==================== å·¥å…·å‡½æ•¸ ====================
# (ä¿æŒä¸è®Š)
def verify_admin(authorization: Optional[str] = Header(None)):
    """é©—è­‰ç®¡ç†å“¡æ¬Šé™"""
    if not ADMIN_PASSWORD or authorization != f"Bearer {ADMIN_PASSWORD}":
        logger.warning(f"ğŸš« ç®¡ç†å“¡æ¬Šé™é©—è­‰å¤±æ•—: {authorization}")
        raise HTTPException(status_code=401, detail="æœªæˆæ¬Šæˆ–æœªè¨­å®šç®¡ç†å“¡å¯†ç¢¼")
    return True

def load_document(file_path: str):
    """è¼‰å…¥æ–‡ä»¶"""
    file_extension = Path(file_path).suffix.lower()
    loader = None
    try:
        if file_extension == ".pdf":
            loader = PyPDFLoader(file_path)
        elif file_extension in [".docx", ".doc"]:
            try:
                import docx2txt # æª¢æŸ¥æ˜¯å¦å®‰è£
                loader = Docx2txtLoader(file_path)
            except ImportError:
                logger.error("âŒ ç¼ºå°‘ 'docx2txt' æ¨¡çµ„ï¼Œç„¡æ³•è®€å– .docx æ–‡ä»¶ã€‚è«‹åŸ·è¡Œ 'pip install docx2txt'")
                return []
        elif file_extension == ".txt":
            try:
                loader = TextLoader(file_path, encoding="utf-8")
                loader.load()[0].page_content[:10]
            except UnicodeDecodeError:
                logger.warning(f"âš ï¸ ä½¿ç”¨ UTF-8 è®€å– {file_path} å¤±æ•—ï¼Œå˜—è©¦ç³»çµ±é è¨­ç·¨ç¢¼...")
                loader = TextLoader(file_path) # ä½¿ç”¨ç³»çµ±é è¨­
            except Exception as txt_err:
                 logger.error(f"âŒ è®€å– TXT æ–‡ä»¶ {file_path} æ™‚ç™¼ç”Ÿéé æœŸçš„éŒ¯èª¤: {txt_err}")
                 return []
        else:
            logger.warning(f"âš ï¸ ä¸æ”¯æ´çš„æª”æ¡ˆé¡å‹: {file_extension} ({file_path})")
            return []

        if loader:
            documents = loader.load()
            logger.info(f"âœ… æˆåŠŸè¼‰å…¥: {file_path} ({len(documents)} æ®µ)")
            return documents
        else:
            return []
    except Exception as e:
        logger.error(f"âŒ è¼‰å…¥æ–‡ä»¶ {file_path} å¤±æ•—: {e}", exc_info=True)
        return []

# ==================== Prompt æ¨¡æ¿ ====================
# (ä¿æŒä¸è®Š)
RAG_PROMPT = PromptTemplate(
    template="""ä½ æ˜¯å¸‚é•·çš„ AI åŠ©ç†ï¼Œè«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™å›ç­”å•é¡Œã€‚

ç›¸é—œè³‡æ–™ï¼š
{context}

å°è©±è¨˜éŒ„ï¼š
{chat_history}

å•é¡Œï¼š{question}

å›ç­”æ™‚è«‹ï¼š
- ä½¿ç”¨å¸‚é•·è¦ªåˆ‡ã€å°ˆæ¥­çš„èªæ°£
- å¦‚æœè³‡æ–™ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹èª å¯¦å‘ŠçŸ¥
- ä¿æŒç°¡æ½”æ˜ç­
- é©æ™‚ä½¿ç”¨è¼•é¬†çš„èªæ°£

å›ç­”ï¼š""",
    input_variables=["context", "chat_history", "question"]
)

# æ›´æ–° ChatService çš„ RAG Prompt
chat_service.rag_prompt = RAG_PROMPT

CONTENT_PROMPT = PromptTemplate(
    template="""ä½ æ˜¯å¸‚é•·çš„å°ˆå±¬æ–‡æ¡ˆç”ŸæˆåŠ©ç†ã€‚

ä»»å‹™ï¼šç”Ÿæˆ {style} é¢¨æ ¼ã€{length} ç¯‡å¹…çš„æ–‡æ¡ˆ

ä¸»é¡Œï¼š{topic}

åƒè€ƒè³‡æ–™ï¼š
{context}

è¦æ±‚ï¼š
- å®Œå…¨æ¨¡ä»¿å¸‚é•·çš„èªæ°£å’Œç”¨è©ç¿’æ…£
- é¢¨æ ¼ï¼š{style}ï¼ˆæ­£å¼/è¼•é¬†/å¹½é»˜ï¼‰
- ç¯‡å¹…ï¼š{length}ï¼ˆçŸ­=50-100å­—ï¼Œä¸­=150-300å­—ï¼Œé•·=400-600å­—ï¼‰
- å…§å®¹å¿…é ˆåŸºæ–¼çŸ¥è­˜åº«è³‡è¨Š
- é¿å…è™›å‡å…§å®¹

æ–‡æ¡ˆå…§å®¹ï¼š""",
    input_variables=["topic", "style", "length", "context"]
)

# ==================== API ç«¯é» ====================

@app.get("/")
async def root():
    return {
        "system": "PAIS æ”¿å‹™åˆ†èº«æ™ºèƒ½ç³»çµ±",
        "version": app.version,
        "framework": "LangChain (Agents + Memory + RAG + Tools)",
        "status": "ğŸŸ¢ é‹è¡Œä¸­"
    }

@app.get("/health")
async def health_check():
    qdrant_ok = False
    llm_ok = False
    agent_ok = agent is not None
    error_msg = ""
    try:
        qdrant_client.get_collections()
        qdrant_ok = True
    except Exception as e:
        error_msg += f"Qdrant é€£æ¥å¤±æ•—: {e}; "
        logger.error(f"âŒ å¥åº·æª¢æŸ¥ - Qdrant é€£æ¥å¤±æ•—: {e}")

    llm_ok = True # æš«æ™‚å‡è¨­ LLM æ­£å¸¸

    status = "healthy" if qdrant_ok and llm_ok and agent_ok else "unhealthy"

    return {
        "status": status,
        "qdrant": "âœ… connected" if qdrant_ok else "âŒ disconnected",
        "llm": "âœ… assumed ok" if llm_ok else "âŒ failed test",
        "agents": "âœ… active" if agent_ok else "âŒ failed to create",
        "error": error_msg if error_msg else None
    }

@app.post("/api/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    å°è©± API (LangChain Agent + Memory æˆ– RAG Chain)
    æ”¯æ´ä¸åŒè§’è‰²ï¼špublic (å–„å¯¶) æˆ– staff (å¹•åƒšåŠ©ç†)

    æ­¤ç«¯é»å·²é‡æ§‹ç‚ºä½¿ç”¨ ChatService è™•ç†æ‰€æœ‰å°è©±é‚è¼¯
    """
    session_id = request.session_id or "default"
    memory = get_memory(session_id)

    try:
        # ä½¿ç”¨ ChatService è™•ç†å°è©±
        result = await chat_service.process_chat(
            message=request.message,
            session_id=session_id,
            memory=memory,
            use_agent=request.use_agent,
            role=request.role
        )

        return ChatResponse(**result)

    except ValueError as e:
        # Agent æœªåˆå§‹åŒ–éŒ¯èª¤
        logger.error(f"âŒ Agent åˆå§‹åŒ–éŒ¯èª¤ ({session_id}): {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        # å…¶ä»–æœªé æœŸéŒ¯èª¤
        logger.error(f"âŒ å°è©±è™•ç†å¤±æ•— ({session_id}): {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å°è©±è™•ç†å¤±æ•—: {str(e)}")

# --- /api/generate ä¿æŒä¸è®Š ---
@app.post("/api/generate")
async def generate_content(
    request: ContentGenerationRequest,
    admin: bool = Depends(verify_admin)
):
    """æ–‡æ¡ˆç”Ÿæˆ API"""
    try:
        logger.info(f"âœï¸ é–‹å§‹ç”Ÿæˆæ–‡æ¡ˆ: ä¸»é¡Œ='{request.topic}', é¢¨æ ¼='{request.style}', é•·åº¦='{request.length}'")

        context = "ï¼ˆç„¡ç‰¹åˆ¥æŒ‡å®šçš„åƒè€ƒè³‡æ–™ï¼‰"
        relevant_docs = []
        if request.context:
             logger.info(f"ğŸ” ä½¿ç”¨æŒ‡å®š context é€²è¡Œç”Ÿæˆ")
             context = request.context
        else:
            try:
                retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
                logger.info(f"ğŸ” å¾çŸ¥è­˜åº«æœå°‹ä¸»é¡Œ '{request.topic}' çš„åƒè€ƒè³‡æ–™...")
                relevant_docs = retriever.invoke(request.topic) # ä½¿ç”¨ invoke
                if relevant_docs:
                    context = "\n\n---\n\n".join([doc.page_content for doc in relevant_docs])
                    logger.info(f"âœ… æ‰¾åˆ° {len(relevant_docs)} ç­†åƒè€ƒè³‡æ–™")
                else:
                    logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ä¸»é¡Œ '{request.topic}' çš„ç›¸é—œåƒè€ƒè³‡æ–™")
            except Exception as search_err:
                 logger.error(f"âŒ æœå°‹åƒè€ƒè³‡æ–™æ™‚å¤±æ•—: {search_err}")

        content_chain = LLMChain(
            llm=llm,
            prompt=CONTENT_PROMPT,
            verbose=True
        )

        logger.info(f"ğŸš€ é–‹å§‹èª¿ç”¨ LLM ç”Ÿæˆæ–‡æ¡ˆ...")
        result = content_chain.invoke({
            "topic": request.topic,
            "style": request.style,
            "length": request.length,
            "context": context
        })

        generated_content = result.get("text", "").strip()

        if not generated_content:
             logger.error("âŒ LLM è¿”å›äº†ç©ºçš„æ–‡æ¡ˆå…§å®¹")
             raise HTTPException(status_code=500, detail="æ–‡æ¡ˆç”Ÿæˆå¤±æ•—ï¼šæ¨¡å‹æœªè¿”å›æœ‰æ•ˆå…§å®¹")

        logger.info(f"âœ… æ–‡æ¡ˆç”ŸæˆæˆåŠŸ (é•·åº¦: {len(generated_content)})")

        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_topic = "".join(c if c.isalnum() else "_" for c in request.topic[:20])
            output_file = Path("generated_content") / f"{timestamp}_{safe_topic}.txt"
            output_file.parent.mkdir(exist_ok=True)

            with open(output_file, "w", encoding="utf-8") as f:
                f.write(f"ä¸»é¡Œ: {request.topic}\n")
                f.write(f"é¢¨æ ¼: {request.style}\n")
                f.write(f"ç¯‡å¹…: {request.length}\n")
                f.write(f"æ™‚é–“: {datetime.now().isoformat()}\n")
                f.write("=" * 50 + "\n\n")
                f.write(generated_content)
            logger.info(f"ğŸ’¾ æ–‡æ¡ˆå·²è‡ªå‹•å„²å­˜è‡³: {output_file}")
            file_path_str = str(output_file)
        except Exception as save_err:
            logger.error(f"âŒ è‡ªå‹•å„²å­˜æ–‡æ¡ˆå¤±æ•—: {save_err}")
            file_path_str = None

        return {
            "content": generated_content,
            "file_path": file_path_str,
            "sources": [
                 doc.metadata.get("source", "æœªçŸ¥ä¾†æº").split('/')[-1]
                 for doc in relevant_docs
            ],
            "context_used": len(relevant_docs) > 0
        }

    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.error(f"âŒ æ–‡æ¡ˆç”Ÿæˆéç¨‹ä¸­ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æ–‡æ¡ˆç”Ÿæˆå¤±æ•—: {str(e)}")


# --- /api/ingest, /api/upload ä¿æŒä¸è®Š ---
@app.post("/api/ingest")
async def ingest_documents(
    request: IngestRequest,
    admin: bool = Depends(verify_admin)
):
    """çŸ¥è­˜åº«ä¸Šå‚³ API (è™•ç†æŒ‡å®šè³‡æ–™å¤¾å…§æ‰€æœ‰æ”¯æ´æ–‡ä»¶)"""
    processed_files_count = 0
    total_chunks_created = 0
    errors = []

    try:
        folder_path = Path(request.folder_path)
        if not folder_path.exists() or not folder_path.is_dir():
            logger.error(f"âŒ Ingest - è³‡æ–™å¤¾ä¸å­˜åœ¨æˆ–ä¸æ˜¯æœ‰æ•ˆç›®éŒ„: {folder_path}")
            raise HTTPException(status_code=404, detail=f"è³‡æ–™å¤¾ '{folder_path}' ä¸å­˜åœ¨")

        supported_extensions = [".pdf", ".docx", ".doc", ".txt"]
        logger.info(f"ğŸ“š é–‹å§‹è™•ç†è³‡æ–™å¤¾: {folder_path}")

        files_to_process = [f for f in folder_path.rglob("*")
                            if f.is_file() and f.suffix.lower() in supported_extensions]

        if not files_to_process:
            logger.warning(f"âš ï¸ è³‡æ–™å¤¾ '{folder_path}' ä¸­æ²’æœ‰æ‰¾åˆ°æ”¯æ´çš„æª”æ¡ˆ ({supported_extensions})")
            return {"message": "è³‡æ–™å¤¾ä¸­æ²’æœ‰æ‰¾åˆ°æ”¯æ´çš„æª”æ¡ˆ", "processed": 0, "chunks_created": 0}

        logger.info(f"ğŸ” æ‰¾åˆ° {len(files_to_process)} å€‹æ”¯æ´çš„æª”æ¡ˆï¼Œé–‹å§‹è¼‰å…¥èˆ‡åˆ†å‰²...")

        all_splits = []
        for file_path in files_to_process:
            logger.debug(f"â³ è™•ç†æª”æ¡ˆ: {file_path}")
            docs = load_document(str(file_path))
            if docs:
                for doc in docs:
                    # ç¢ºä¿è·¯å¾‘æ­£ç¢ºè§£æ
                    try:
                        absolute_path = file_path.resolve()
                        relative_path = absolute_path.relative_to(Path.cwd().resolve())
                    except ValueError:
                        # å¦‚æœç„¡æ³•è¨ˆç®—ç›¸å°è·¯å¾‘ï¼Œä½¿ç”¨æª”æ¡ˆè·¯å¾‘æœ¬èº«
                        relative_path = file_path
                    doc.metadata["source"] = str(relative_path).replace("\\", "/")
                    doc.metadata["uploaded_at"] = datetime.now().isoformat()
                    doc.metadata["filename"] = file_path.name

                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000,
                    chunk_overlap=200,
                    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", "ã€", " ", ""],
                    length_function=len,
                )
                splits = text_splitter.split_documents(docs)
                logger.info(f"ğŸ“„ æª”æ¡ˆ {file_path.name} åˆ†å‰²æˆ {len(splits)} å€‹ç‰‡æ®µ")
                all_splits.extend(splits)
                processed_files_count += 1
            else:
                 logger.warning(f"âš ï¸ æª”æ¡ˆ {file_path} è¼‰å…¥å¤±æ•—æˆ–ç„¡å…§å®¹ï¼Œå·²è·³é")
                 errors.append(f"ç„¡æ³•è™•ç†æª”æ¡ˆ: {file_path.name}")


        if not all_splits:
            logger.error("âŒ ç„¡æ³•å¾ä»»ä½•æª”æ¡ˆä¸­æˆåŠŸåˆ†å‰²å‡ºç‰‡æ®µ")
            return {"message": "ç„¡æ³•æˆåŠŸè™•ç†ä»»ä½•æ–‡ä»¶å…§å®¹", "processed": processed_files_count, "chunks_created": 0, "errors": errors}

        total_chunks_created = len(all_splits)
        logger.info(f"âœ… æ–‡ä»¶åˆ†å‰²å®Œæˆ: å…± {processed_files_count} å€‹æª”æ¡ˆ, {total_chunks_created} å€‹ç‰‡æ®µã€‚æº–å‚™å¯«å…¥å‘é‡è³‡æ–™åº«...")

        batch_size = 100
        for i in range(0, total_chunks_created, batch_size):
            batch = all_splits[i:i + batch_size]
            try:
                vectorstore.add_documents(batch)
                logger.info(f"âœï¸ å·²å¯«å…¥ {len(batch)} å€‹ç‰‡æ®µ (ç¸½é€²åº¦: {min(i + batch_size, total_chunks_created)} / {total_chunks_created})")
            except Exception as add_doc_err:
                 logger.error(f"âŒ å¯«å…¥ç‰‡æ®µ {i} åˆ° {i+batch_size} æ™‚å¤±æ•—: {add_doc_err}", exc_info=True)
                 errors.append(f"éƒ¨åˆ†è³‡æ–™å¯«å…¥å¤±æ•—: {add_doc_err}")

        logger.info(f"âœ… æ‰€æœ‰ç‰‡æ®µå·²æˆåŠŸå¯«å…¥å‘é‡è³‡æ–™åº« '{COLLECTION_NAME}'")

        return {
            "message": "âœ… çŸ¥è­˜åº«æ›´æ–°æˆåŠŸ" + (f" (éƒ¨åˆ†æª”æ¡ˆè™•ç†å¤±æ•—ï¼Œè«‹æŸ¥çœ‹æ—¥èªŒ)" if errors else ""),
            "files_processed": processed_files_count,
            "chunks_created": total_chunks_created,
            "collection": COLLECTION_NAME,
            "errors": errors if errors else None
        }

    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.error(f"âŒ çŸ¥è­˜åº«ä¸Šå‚³éç¨‹ä¸­ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"çŸ¥è­˜åº«è™•ç†å¤±æ•—: {str(e)}")

@app.post("/api/upload")
async def upload_file(
    file: UploadFile = File(...),
    folder: str = Form(""),
    admin: bool = Depends(verify_admin)
):
    """å–®å€‹æª”æ¡ˆä¸Šå‚³ä¸¦ç›´æ¥åŠ å…¥çŸ¥è­˜åº«"""
    try:
        # å»ºç«‹ä¸Šå‚³ç›®æ¨™è³‡æ–™å¤¾
        upload_folder = Path("documents")
        if folder and folder.strip():
            # æ¸…ç†è³‡æ–™å¤¾è·¯å¾‘ï¼Œé˜²æ­¢è·¯å¾‘éæ­·æ”»æ“Š
            clean_folder = folder.strip().replace("..", "").replace("\\", "/")
            upload_folder = upload_folder / clean_folder

        upload_folder.mkdir(parents=True, exist_ok=True)
        file_path = upload_folder / Path(file.filename).name

        if file_path.exists():
             timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
             file_path = upload_folder / f"{file_path.stem}_{timestamp}{file_path.suffix}"

        logger.info(f"ğŸ“¤ æ¥æ”¶åˆ°æª”æ¡ˆä¸Šå‚³: {file.filename}, å„²å­˜è‡³: {file_path}")

        try:
            with open(file_path, "wb") as buffer:
                content = await file.read()
                buffer.write(content)
            logger.info(f"ğŸ’¾ æª”æ¡ˆå„²å­˜æˆåŠŸ: {file_path}")
        except Exception as save_err:
             logger.error(f"âŒ å„²å­˜ä¸Šå‚³æª”æ¡ˆå¤±æ•— ({file.filename}): {save_err}", exc_info=True)
             raise HTTPException(status_code=500, detail=f"å„²å­˜æª”æ¡ˆå¤±æ•—: {save_err}")

        logger.info(f"ğŸ“š é–‹å§‹è™•ç†å–®ä¸€æª”æ¡ˆ: {file_path}")
        docs = load_document(str(file_path))

        if not docs:
            logger.warning(f"âš ï¸ æª”æ¡ˆ {file_path} è¼‰å…¥å¤±æ•—æˆ–ç„¡å…§å®¹ï¼Œç„¡æ³•åŠ å…¥çŸ¥è­˜åº«")
            return {
                "message": "æª”æ¡ˆå·²æˆåŠŸä¸Šå‚³ï¼Œä½†ç„¡æ³•è®€å–å…§å®¹æˆ–å…§å®¹ç‚ºç©ºï¼ŒæœªåŠ å…¥çŸ¥è­˜åº«ã€‚",
                "filename": file.filename,
                "chunks": 0,
                "error": "Failed to load or empty document"
            }

        for doc in docs:
            # ç¢ºä¿ file_path æ˜¯çµ•å°è·¯å¾‘ï¼Œç„¶å¾Œè¨ˆç®—ç›¸å°æ–¼å·¥ä½œç›®éŒ„çš„è·¯å¾‘
            abs_file_path = file_path.resolve()
            abs_cwd = Path.cwd().resolve()
            try:
                relative_path = abs_file_path.relative_to(abs_cwd)
            except ValueError:
                # å¦‚æœç„¡æ³•è¨ˆç®—ç›¸å°è·¯å¾‘ï¼Œç›´æ¥ä½¿ç”¨æª”æ¡ˆè·¯å¾‘
                relative_path = file_path

            doc.metadata["source"] = str(relative_path).replace("\\", "/")
            doc.metadata["uploaded_at"] = datetime.now().isoformat()
            doc.metadata["filename"] = file_path.name

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", "ã€", " ", ""],
            length_function=len
        )
        splits = text_splitter.split_documents(docs)
        total_chunks = len(splits)
        logger.info(f"ğŸ“„ æª”æ¡ˆ {file_path.name} åˆ†å‰²æˆ {total_chunks} å€‹ç‰‡æ®µ")

        if not splits:
             logger.warning(f"âš ï¸ æª”æ¡ˆ {file_path.name} åˆ†å‰²å¾Œç„¡ç‰‡æ®µï¼Œç„¡æ³•åŠ å…¥çŸ¥è­˜åº«")
             return {
                "message": "æª”æ¡ˆå·²æˆåŠŸä¸Šå‚³ï¼Œä½†åˆ†å‰²å¾Œç„¡æœ‰æ•ˆå…§å®¹ï¼ŒæœªåŠ å…¥çŸ¥è­˜åº«ã€‚",
                "filename": file.filename,
                "chunks": 0,
                "error": "No chunks generated after splitting"
            }

        try:
            vectorstore.add_documents(splits)
            logger.info(f"âœ… æª”æ¡ˆ {file_path.name} çš„ç‰‡æ®µå·²æˆåŠŸåŠ å…¥å‘é‡è³‡æ–™åº«")
            return {
                "message": "âœ… æª”æ¡ˆä¸Šå‚³ä¸¦æˆåŠŸåŠ å…¥çŸ¥è­˜åº«",
                "filename": file.filename,
                "chunks": total_chunks
            }
        except Exception as add_doc_err:
            logger.error(f"âŒ å°‡æª”æ¡ˆ {file_path.name} åŠ å…¥å‘é‡è³‡æ–™åº«æ™‚å¤±æ•—: {add_doc_err}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"åŠ å…¥çŸ¥è­˜åº«å¤±æ•—: {add_doc_err}")

    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.error(f"âŒ è™•ç†æª”æ¡ˆä¸Šå‚³æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤ ({file.filename}): {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æª”æ¡ˆä¸Šå‚³è™•ç†å¤±æ•—: {str(e)}")


# --- å…¶ä»– API ä¿æŒä¸è®Š ---
@app.get("/api/memory/{session_id}")
async def get_memory_history(session_id: str):
    """å–å¾—æŒ‡å®š session çš„å°è©±è¨˜æ†¶"""
    try:
        if session_id not in memory_store:
            history_file = Path(f"chat_history/{session_id}.json")
            if not history_file.exists():
                logger.warning(f"âš ï¸ è«‹æ±‚è¨˜æ†¶é«”æ­·å²ï¼Œä½† session '{session_id}' ä¸å­˜åœ¨ (è¨˜æ†¶é«”èˆ‡æª”æ¡ˆçš†ç„¡)")
                raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°æ­¤å°è©±è¨˜éŒ„")
            else:
                 get_memory(session_id)

        memory = memory_store[session_id]
        history_data = memory.load_memory_variables({})
        messages = history_data.get("chat_history", [])
        formatted_history = []
        for msg in messages:
            msg_type = "user" if msg.type == "human" else "ai" if msg.type == "ai" else "system"
            formatted_history.append({
                "type": msg_type,
                "content": msg.content
            })

        logger.info(f"âœ… æˆåŠŸå–å¾— session '{session_id}' çš„å°è©±æ­·å² ({len(formatted_history)} æ¢)")
        return {
            "session_id": session_id,
            "history": formatted_history
        }
    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.error(f"âŒ å–å¾—å°è©±æ­·å²å¤±æ•— ({session_id}): {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç„¡æ³•å–å¾—å°è©±è¨˜éŒ„: {str(e)}")

@app.delete("/api/memory/{session_id}")
async def clear_memory(session_id: str, admin: bool = Depends(verify_admin)):
    """æ¸…é™¤æŒ‡å®š session çš„å°è©±è¨˜æ†¶ (è¨˜æ†¶é«”èˆ‡æª”æ¡ˆ)"""
    try:
        deleted_from_memory = False
        deleted_from_file = False

        if session_id in memory_store:
            try:
                memory_store[session_id].clear()
                del memory_store[session_id]
                deleted_from_memory = True
                logger.info(f"ğŸ—‘ï¸ å·²å¾è¨˜æ†¶é«”ä¸­æ¸…é™¤ session: {session_id}")
            except Exception as mem_clear_err:
                 logger.error(f"âŒ æ¸…é™¤è¨˜æ†¶é«”ä¸­ session '{session_id}' å¤±æ•—: {mem_clear_err}")

        history_file = Path(f"chat_history/{session_id}.json")
        if history_file.exists():
            try:
                history_file.unlink()
                deleted_from_file = True
                logger.info(f"ğŸ—‘ï¸ å·²åˆªé™¤å°è©±æ­·å²æª”æ¡ˆ: {history_file}")
            except OSError as file_del_err:
                logger.error(f"âŒ åˆªé™¤å°è©±æ­·å²æª”æ¡ˆ '{history_file}' å¤±æ•—: {file_del_err}")

        if deleted_from_memory or deleted_from_file:
            return {"message": f"âœ… å·²æ¸…é™¤ {session_id} çš„å°è©±è¨˜æ†¶" + (" (éƒ¨åˆ†å¤±æ•—ï¼Œè«‹æŸ¥çœ‹æ—¥èªŒ)" if not (deleted_from_memory and deleted_from_file) else "")}
        else:
            logger.warning(f"âš ï¸ å˜—è©¦æ¸…é™¤ session '{session_id}'ï¼Œä½†è¨˜æ†¶é«”èˆ‡æª”æ¡ˆçš†ä¸å­˜åœ¨æˆ–åˆªé™¤å¤±æ•—")
            raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°æˆ–ç„¡æ³•æ¸…é™¤æŒ‡å®šçš„å°è©±è¨˜éŒ„")

    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.error(f"âŒ æ¸…é™¤å°è©±è¨˜æ†¶æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤ ({session_id}): {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æ¸…é™¤è¨˜æ†¶å¤±æ•—: {str(e)}")


@app.get("/api/documents")
async def list_documents(admin: bool = Depends(verify_admin)):
    """åˆ—å‡ºçŸ¥è­˜åº«ä¸­çš„æ‰€æœ‰æ–‡æª”"""
    try:
        docs_dir = Path("documents")
        if not docs_dir.exists():
            return {"documents": []}

        documents = []

        # éæ­·æ‰€æœ‰æ–‡ä»¶ï¼ˆåŒ…æ‹¬å­ç›®éŒ„ï¼‰
        for file_path in docs_dir.rglob("*"):
            if file_path.is_file():
                try:
                    stat_info = file_path.stat()
                    relative_path = file_path.relative_to(docs_dir)

                    documents.append({
                        "filename": file_path.name,
                        "path": str(relative_path).replace("\\", "/"),
                        "full_path": str(file_path),
                        "size": stat_info.st_size,
                        "uploaded_at": datetime.fromtimestamp(stat_info.st_mtime).isoformat(),
                        "extension": file_path.suffix
                    })
                except Exception as file_err:
                    logger.warning(f"âš ï¸ ç„¡æ³•è®€å–æ–‡ä»¶è³‡è¨Š: {file_path}, éŒ¯èª¤: {file_err}")
                    continue

        # æŒ‰ä¸Šå‚³æ™‚é–“æ’åºï¼ˆæ–°åˆ°èˆŠï¼‰
        documents.sort(key=lambda x: x["uploaded_at"], reverse=True)

        logger.info(f"ğŸ“‚ åˆ—å‡ºæ–‡æª”åˆ—è¡¨ï¼Œå…± {len(documents)} å€‹æ–‡ä»¶")
        return {
            "documents": documents,
            "total": len(documents)
        }
    except Exception as e:
        logger.error(f"âŒ åˆ—å‡ºæ–‡æª”å¤±æ•—: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"åˆ—å‡ºæ–‡æª”å¤±æ•—: {str(e)}")


@app.get("/api/documents/{file_path:path}/download")
async def download_document(file_path: str):
    """ä¸‹è¼‰çŸ¥è­˜åº«ä¸­çš„æ–‡æª”ï¼ˆä¸éœ€è¦å¯†ç¢¼é©—è­‰ï¼Œæ–¹ä¾¿ä¸‹è¼‰ï¼‰"""
    try:
        from fastapi.responses import FileResponse

        docs_dir = Path("documents")
        target_file = docs_dir / file_path

        # å®‰å…¨æª¢æŸ¥ï¼šç¢ºä¿æ–‡ä»¶åœ¨ documents ç›®éŒ„å…§
        try:
            target_file = target_file.resolve()
            docs_dir = docs_dir.resolve()
            if not str(target_file).startswith(str(docs_dir)):
                raise HTTPException(status_code=400, detail="ä¸å…è¨±è¨ªå•æ­¤è·¯å¾‘")
        except Exception:
            raise HTTPException(status_code=400, detail="ç„¡æ•ˆçš„æ–‡ä»¶è·¯å¾‘")

        if not target_file.exists():
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")

        if not target_file.is_file():
            raise HTTPException(status_code=400, detail="åªèƒ½ä¸‹è¼‰æ–‡ä»¶")

        logger.info(f"ğŸ“¥ ä¸‹è¼‰æ–‡æª”: {file_path}")

        return FileResponse(
            path=str(target_file),
            filename=target_file.name,
            media_type='application/octet-stream'
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ä¸‹è¼‰æ–‡æª”å¤±æ•— ({file_path}): {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ä¸‹è¼‰æ–‡æª”å¤±æ•—: {str(e)}")


@app.delete("/api/documents/{file_path:path}")
async def delete_document(file_path: str, admin: bool = Depends(verify_admin)):
    """åˆªé™¤çŸ¥è­˜åº«ä¸­çš„æ–‡æª”"""
    try:
        docs_dir = Path("documents")
        target_file = docs_dir / file_path

        # å®‰å…¨æª¢æŸ¥ï¼šç¢ºä¿æ–‡ä»¶åœ¨ documents ç›®éŒ„å…§
        try:
            target_file = target_file.resolve()
            docs_dir = docs_dir.resolve()
            if not str(target_file).startswith(str(docs_dir)):
                raise HTTPException(status_code=400, detail="ä¸å…è¨±è¨ªå•æ­¤è·¯å¾‘")
        except Exception:
            raise HTTPException(status_code=400, detail="ç„¡æ•ˆçš„æ–‡ä»¶è·¯å¾‘")

        if not target_file.exists():
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")

        if not target_file.is_file():
            raise HTTPException(status_code=400, detail="åªèƒ½åˆªé™¤æ–‡ä»¶ï¼Œä¸èƒ½åˆªé™¤ç›®éŒ„")

        # åˆªé™¤æ–‡ä»¶
        filename = target_file.name
        target_file.unlink()
        logger.info(f"ğŸ—‘ï¸ å·²åˆªé™¤æ–‡æª”: {file_path}")

        return {
            "message": f"âœ… å·²åˆªé™¤æ–‡æª”: {filename}",
            "filename": filename
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ åˆªé™¤æ–‡æª”å¤±æ•— ({file_path}): {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"åˆªé™¤æ–‡æª”å¤±æ•—: {str(e)}")


@app.get("/api/stats")
async def get_stats():
    """å–å¾—ç³»çµ±çµ±è¨ˆè³‡è¨Š"""
    try:
        vector_count = -1
        try:
            collection_info = qdrant_client.get_collection(COLLECTION_NAME)
            vector_count = collection_info.vectors_count
        except Exception as q_err:
             logger.error(f"âŒ ç„¡æ³•å¾ Qdrant å–å¾—é›†åˆè³‡è¨Š: {q_err}")

        return {
            "collection_name": COLLECTION_NAME,
            "total_vectors": vector_count,
            "active_memory_sessions": len(memory_store),
            "total_history_files": len(list(Path("chat_history").glob("*.json"))),
            "framework": "LangChain",
            "llm_model": llm.model,
            "embedding_model": embeddings.model_name,
            "vector_db": "Qdrant",
            "components": {
                "agents": "âœ… ReAct Agent" if agent else "âŒ Agent Failed",
                "memory": "âœ… ConversationBufferMemory + FileChatMessageHistory",
                "rag": "âœ… ConversationalRetrievalChain",
                "tools": len(tools)
            }
        }
    except Exception as e:
        logger.error(f"âŒ å–å¾—ç³»çµ±çµ±è¨ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç„¡æ³•å–å¾—ç³»çµ±çµ±è¨ˆ: {str(e)}")


# ==================== è¨ªå®¢è¨ˆæ•¸å™¨ API ====================

@app.post("/api/visitor/increment", response_model=VisitorStatsResponse)
async def increment_visitor():
    """å¢åŠ è¨ªå®¢è¨ˆæ•¸"""
    try:
        stats = db.increment_visitor_count()
        return VisitorStatsResponse(
            month=stats['month'],
            count=stats['count'],
            last_reset=stats['last_reset']
        )
    except Exception as e:
        logger.error(f"âŒ å¢åŠ è¨ªå®¢è¨ˆæ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç„¡æ³•å¢åŠ è¨ªå®¢è¨ˆæ•¸: {str(e)}")


@app.get("/api/visitor/stats", response_model=VisitorStatsResponse)
async def get_visitor_stats(month: Optional[str] = None):
    """å–å¾—è¨ªå®¢çµ±è¨ˆ"""
    try:
        stats = db.get_visitor_stats(month)
        if stats:
            return VisitorStatsResponse(
                month=stats['month'],
                count=stats['count'],
                last_reset=stats['last_reset']
            )
        else:
            # å¦‚æœæ²’æœ‰æ•¸æ“šï¼Œè¿”å›ç•¶æœˆåˆå§‹å€¼
            from datetime import datetime
            current_month = month or datetime.now().strftime("%Y-%m")
            return VisitorStatsResponse(
                month=current_month,
                count=0,
                last_reset=datetime.now().isoformat()
            )
    except Exception as e:
        logger.error(f"âŒ å–å¾—è¨ªå®¢çµ±è¨ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç„¡æ³•å–å¾—è¨ªå®¢çµ±è¨ˆ: {str(e)}")


# --- å•Ÿå‹•èˆ‡é—œé–‰äº‹ä»¶ä¿æŒä¸è®Š ---
@app.on_event("startup")
async def startup_event():
    Path("chat_history").mkdir(exist_ok=True)
    Path("generated_content").mkdir(exist_ok=True)
    Path("logs").mkdir(exist_ok=True)
    Path("documents").mkdir(exist_ok=True)

    logger.info("="*50)
    logger.info("ğŸš€ PAIS ç³»çµ±å•Ÿå‹• (LangChain Powered)")
    logger.info(f" FastAPI ç‰ˆæœ¬: {app.version}")
    logger.info(f"ğŸ“ Qdrant Host: {QDRANT_HOST}:{QDRANT_PORT}")
    logger.info(f"ğŸ“š Qdrant é›†åˆ: {COLLECTION_NAME}")
    logger.info(f"ğŸ§  LLM æ¨¡å‹: {llm.model}")
    logger.info(f"ğŸ”¡ Embedding æ¨¡å‹: {embeddings.model_name} (ç¶­åº¦: 768)")
    logger.info(f"ğŸ¤– Agent ç‹€æ…‹: {'âœ… å·²å•Ÿç”¨' if agent else 'âŒ å•Ÿå‹•å¤±æ•—'}")
    logger.info(f"ğŸ› ï¸ Agent å·¥å…·æ•¸é‡: {len(tools)}")
    logger.info(f"ğŸ’¾ Memory é¡å‹: ConversationBufferMemory + FileChatMessageHistory")
    logger.info("="*50)

@app.on_event("shutdown")
async def shutdown_event():
    logger.info("="*50)
    logger.info("â¹ PAIS ç³»çµ±æ­£åœ¨é—œé–‰...")
    logger.info("="*50)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=False)